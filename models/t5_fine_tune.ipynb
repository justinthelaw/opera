import os
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from loguru import logger

def load_and_preprocess_data():
    # Load the training data
    data = torch.load('data/training/data.pt')

    # Tokenize the data using the T5 tokenizer
    tokenizer = T5Tokenizer.from_pretrained('t5-base')
    inputs = tokenizer(data, return_tensors='pt', padding=True, truncation=True)

    return inputs

def train_t5_model():
    # Initialize the T5 model
    model = T5ForConditionalGeneration.from_pretrained('t5-base')

    # Define the loss function and optimizer
    loss_fn = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())

    # Load and preprocess the training data
    inputs = load_and_preprocess_data()

    # Train the model
    for epoch in range(10):  # number of epochs can be adjusted
        optimizer.zero_grad()
        outputs = model(**inputs)
        loss = loss_fn(outputs.logits, inputs['input_ids'])
        loss.backward()
        optimizer.step()

try:
    # Load and preprocess the training data
    load_and_preprocess_data()

    # Train the T5 model
    train_t5_model()

    # Save the fine-tuned model
    save_fine_tuned_model()

except Exception as e:
    logger.error(f"Error occurred: {e}")

def save_fine_tuned_model():
    # Save the fine-tuned model to a specified directory
    model.save_model('models/t5_fine_tuned')
    # Save the fine-tuned model
    save_fine_tuned_model()
