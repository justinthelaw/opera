{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Forge Model Fine Tuning (v1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Instantiate Global Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Max length of tokens a user may enter for summarization\n",
        "Increasing this beyond 512 may increase compute time significantly\n",
        "\"\"\"\n",
        "max_input_token_length = 512\n",
        "\"\"\"\n",
        "Max length of tokens the model should output for the summary\n",
        "Approximately the number of tokens it may take to generate a bullet\n",
        "\"\"\"\n",
        "max_output_token_length = 128\n",
        "\"\"\"\n",
        "Beams to use for beam search algorithm\n",
        "Increased beams means increased quality, but increased compute time\n",
        "\"\"\"\n",
        "number_of_beams = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The block below allows the user to pull in the pre-trained and/or raw model checkpoint into this repository's local _forge/models/_ directory. This step is optional, but it allows [Step 2](#step-2-fine-tune-t5-checkpoint-model)'s user input to be a model in your local directory, thus providing offline usage and fine tuning later on. E.g., if you download google/flan-t5-xl to the local directory with name 'my-test-model' first, you can input '../models/my-test-model' for executing fine tuning on. The script below works on any model and tokenizer, but the fine tuning script in [Step 2](#step-2-fine-tune-t5-checkpoint-model) depends on the usage of a T5x variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Specify the model name\n",
        "model_name = input(\n",
        "    \"What is the target model's checkpoint name on Hugging Face?\"\n",
        ")\n",
        "\n",
        "# Specify your directory path\n",
        "directory_path = f\"../models/{model_name}\"\n",
        "\n",
        "# Download and load the pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Save the model and tokenizer to your specified directory\n",
        "tokenizer.save_pretrained(directory_path)\n",
        "model.save_pretrained(directory_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fine Tune T5 Checkpoint Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T5 Variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [],
      "source": [
        "# Fine tuning scripts\n",
        "import signal\n",
        "import re\n",
        "import traceback\n",
        "from loguru import logger\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "from scripts.file_utils import load_jsonl_data\n",
        "from scripts.constants import *\n",
        "from scripts.rich_logger import training_table as table, live_refresher as refresher\n",
        "\n",
        "\n",
        "input_model = input(\n",
        "    \"What is the target model's relative directory path or checkpoint name on Hugging Face?\"\n",
        ")\n",
        "\n",
        "# Model fine tuning parameter control object\n",
        "model_params = {\n",
        "    # Name of the pre-trained model or checkpoint name that will be fine-tuned\n",
        "    \"MODEL\": f\"{input_model}\",\n",
        "    \"TOKENIZER\": \"google/flan-t5-base\",\n",
        "    # Number of examples per batch during training\n",
        "    # Larger batch sizes require more memory, but can speed up training\n",
        "    \"TRAIN_BATCH_SIZE\": 1,\n",
        "    # Number of full passes through the entire training dataset\n",
        "    # More epochs can lead to better performance, but risk over-fitting\n",
        "    \"TRAIN_EPOCHS\": 4,\n",
        "    # Number of examples per batch during validation\n",
        "    # Larger batch sizes require more memory, but can speed up the validation process\n",
        "    \"VALID_BATCH_SIZE\": 1,\n",
        "    # Number of full passes through the entire validation dataset\n",
        "    # Typically kept to a single epoch as the validation set does not need to be repeatedly passed\n",
        "    \"VAL_EPOCHS\": 1,\n",
        "    # Affects how quickly or slowly a model learns\n",
        "    # Too high can cause instability, too low can cause slow learning\n",
        "    \"LEARNING_RATE\": 1e-7,\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": max_input_token_length,\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": max_output_token_length,\n",
        "    # Random seed to ensure reproducibility\n",
        "    # Using the same seed will yield the same model given the same data and training process\n",
        "    \"SEED\": 166,\n",
        "    \"NUM_BEAMS\": number_of_beams,\n",
        "    # Multiplier to penalize repeated n-grams\n",
        "    # Higher values discourage repetition in the generated text\n",
        "    \"REPETITION_PENALTY\": 0.5,\n",
        "    # Penalty applied for producing long sequences\n",
        "    # Higher values encourage longer sequences\n",
        "    \"LENGTH_PENALTY\": 0.5,\n",
        "    # The number of steps to take before the gradient is averaged and applied\n",
        "    # Helps in stabilizing training and requires less memory\n",
        "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
        "    # Weight decay introduced to the optimizer to prevent over-fitting\n",
        "    # Regularization strategy by adding a small penalty, typically the L2 norm of the weights\n",
        "    \"WEIGHT_DECAY\": 0.0,\n",
        "    # Small constant to prevent any division by zero in the implementation (Adam)\n",
        "    \"ADAM_EPSILON\": 1e-8,\n",
        "    # Number of steps for the warmup phase\n",
        "    # Helps in avoiding very high and undesirable values of gradients at the start of training\n",
        "    \"WARMUP_STEPS\": 3,\n",
        "    # The split between the training and validation data\n",
        "    \"TRAINING_VALIDATION_SPLIT\": 0.85,\n",
        "}\n",
        "\n",
        "model_output_directory = \"../models/\" + input(\n",
        "    \"What name would you like to give the fine-tuned model?\"\n",
        ")\n",
        "\n",
        "prompt_prefix_option = input(\n",
        "    \"Type the number to choose a prompt prefix type: (1) Bullet Prompt Training or (2) Data Creation Training\"\n",
        ")\n",
        "prompt_prefix = (\n",
        "    bullet_data_creation_prefix if prompt_prefix_option == \"2\" else bullet_prompt_prefix\n",
        ")\n",
        "data_set = (\n",
        "    \"../data/training/data_creation_set.jsonl\"\n",
        "    if prompt_prefix_option == \"2\"\n",
        "    else \"../data/training/training_validation_set.jsonl\"\n",
        ")\n",
        "\n",
        "data = load_jsonl_data(\n",
        "    data_set,\n",
        "    prompt_prefix,\n",
        "    isDataFrame=True,\n",
        ")\n",
        "\n",
        "# Set device to be used based on GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "logger.info(f\"Loading {model_params['MODEL']}...\")\n",
        "# Model is sent to device (GPU/TPU) for using the hardware\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "model = model.to(device)\n",
        "# Tokenzier for encoding the text\n",
        "tokenizer = T5Tokenizer.from_pretrained(\n",
        "    model_params[\"TOKENIZER\"], model_max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Creating a custom dataset for reading the dataset and loading it into the dataloader\n",
        "# to pass it to the neural network for fine tuning the model\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # Cleaning data so as to ensure data is in string type\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "# Generates a penalty for not complying to bullet formatting\n",
        "def format_penalty(outputs, tokenizer, format_pattern):\n",
        "    total_penalty = 0.0\n",
        "    logits = outputs.logits\n",
        "    # Converting the logits to token ids\n",
        "    token_ids = torch.argmax(logits, dim=-1)\n",
        "    # Decoding the token ids to text\n",
        "    decoded_outputs = [\n",
        "        tokenizer.decode(token_ids[i], skip_special_tokens=True)\n",
        "        for i in range(token_ids.shape[0])\n",
        "    ]\n",
        "\n",
        "    for text in decoded_outputs:\n",
        "        match = re.fullmatch(format_pattern, text)\n",
        "        # If the output does not match the desired format exactly, add a penalty\n",
        "        if not match:\n",
        "            total_penalty += 1.0\n",
        "\n",
        "    return torch.tensor(total_penalty, device=logits.device)\n",
        "\n",
        "\n",
        "# Function to be called for training with the parameters passed from main function\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer, scheduler):\n",
        "    # Create a GradScaler object for mixed precision training\n",
        "    scaler = GradScaler()\n",
        "    # Training logger refresh flag\n",
        "    table.switch_epoch_refresh()\n",
        "    # Stepping through training batches\n",
        "    for step, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                decoder_input_ids=y_ids,\n",
        "                labels=lm_labels,\n",
        "            )\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Add a penalty to the loss for outputs that don't match the format\n",
        "            format_loss = format_penalty(outputs, tokenizer, bullet_pattern)\n",
        "            total_loss = loss + format_loss\n",
        "\n",
        "            if table.get_epoch_refresh():\n",
        "                # Refresh table once per epoch\n",
        "                table.refresh_table(epoch, loss)\n",
        "\n",
        "            # Backward pass with mixed precision\n",
        "            scaler.scale(total_loss).backward()\n",
        "\n",
        "            # Check if the accumulated gradients are ready to be applied and the optimizer should be updated\n",
        "            if (step + 1) % model_params[\"GRADIENT_ACCUMULATION_STEPS\"] == 0:\n",
        "                # Unscale the gradients to allow proper gradient scaling with mixed precision\n",
        "                scaler.unscale_(optimizer)\n",
        "                # Clip the gradients to prevent \"exploding gradients\" problem\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                # Back-propagate the scaled loss to compute the gradients\n",
        "                scaler.step(optimizer)\n",
        "                # Update the scaler's scale factor for the next iteration\n",
        "                scaler.update()\n",
        "\n",
        "            # Clear gradients to avoid accumulation of gradients from previous batches\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Adjust the learning rate based on the scheduler's update policy\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "# Function to evaluate model for predictions and compute ROUGE scores\n",
        "def validate(tokenizer, model, device, loader):\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    # Initialize the rouge scorer\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "                num_beams=model_params[\"NUM_BEAMS\"],\n",
        "                repetition_penalty=model_params[\"REPETITION_PENALTY\"],\n",
        "                length_penalty=model_params[\"LENGTH_PENALTY\"],\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            preds = [\n",
        "                tokenizer.decode(\n",
        "                    g, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for g in generated_ids\n",
        "            ]\n",
        "            targets = [\n",
        "                tokenizer.decode(\n",
        "                    t, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for t in y\n",
        "            ]\n",
        "\n",
        "            # Calculate rouge scores for each prediction and corresponding target\n",
        "            for pred, target in zip(preds, targets):\n",
        "                score = scorer.score(target, pred)\n",
        "                scores.append(score)\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(targets)\n",
        "\n",
        "    # Compute the average ROUGE scores for the entire validation set\n",
        "    avg_scores = {\n",
        "        \"rouge1\": np.mean([score[\"rouge1\"].fmeasure for score in scores]),\n",
        "        \"rouge2\": np.mean([score[\"rouge2\"].fmeasure for score in scores]),\n",
        "        \"rougeL\": np.mean([score[\"rougeL\"].fmeasure for score in scores]),\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Average ROUGE scores: {avg_scores}\")\n",
        "\n",
        "    return predictions, actuals\n",
        "\n",
        "\n",
        "# T5 training main function\n",
        "def T5Trainer(dataframe, source_text, target_text):\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])\n",
        "    np.random.seed(model_params[\"SEED\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    logger.info(\"Reading data...\")\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # 80% of the data will be used for training and the rest for validation\n",
        "    train_size = model_params[\"TRAINING_VALIDATION_SPLIT\"]\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    logger.info(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    logger.info(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    logger.info(f\"VALIDATION Dataset: {val_dataset.shape}\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of data loader\n",
        "    training_set = CustomDataset(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = CustomDataset(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of data loaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of data loaders for testing and validation - this will be used down for training and validation stage for the model\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        params=[p for p in model.parameters() if p.requires_grad],\n",
        "        lr=model_params[\"LEARNING_RATE\"],\n",
        "        eps=model_params[\"ADAM_EPSILON\"],\n",
        "        weight_decay=model_params[\"WEIGHT_DECAY\"],\n",
        "    )\n",
        "\n",
        "    # Define the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=model_params[\"WARMUP_STEPS\"],\n",
        "        num_training_steps=model_params[\"TRAIN_EPOCHS\"]\n",
        "        * len(training_loader)\n",
        "        // model_params[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    logger.info(f\"Initiating fine tuning of {model_params['MODEL']}...\")\n",
        "    # Table logger for training statistics\n",
        "    with refresher:\n",
        "        for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "            train(\n",
        "                epoch, tokenizer, model, device, training_loader, optimizer, scheduler\n",
        "            )\n",
        "    logger.info(f\"Saving fine-tuned  to {model_output_directory} ...\")\n",
        "    # Saving the model after training\n",
        "    save_model()\n",
        "\n",
        "    # Evaluating validation dataset\n",
        "    logger.info(\"Initiating validation...\")\n",
        "    for _ in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        validate(tokenizer, model, device, val_loader)\n",
        "    logger.success(\"Model fine tuning, saving, and validation steps completed!\")\n",
        "\n",
        "\n",
        "# Saves the model\n",
        "def save_model():\n",
        "    model.save_pretrained(model_output_directory)\n",
        "    tokenizer.save_pretrained(model_output_directory)\n",
        "    logger.info(f\"Fine-tuned model successfully saved to: {model_output_directory}\")\n",
        "    logger.success(\"Model saved. Shutting down...\")\n",
        "\n",
        "\n",
        "# In case of interrupt, save model and exit\n",
        "def save_and_exit(signal, _):\n",
        "    logger.warning(\n",
        "        f\"Received interrupt signal {signal}, stopping script and saving model...\"\n",
        "    )\n",
        "    save_model()\n",
        "\n",
        "\n",
        "# Attach the SIGINT signal (generated by Ctrl+C) to the handler\n",
        "signal.signal(signal.SIGINT, save_and_exit)\n",
        "\n",
        "try:\n",
        "    # Run training function on the T5 model using data set and training parameters\n",
        "    T5Trainer(dataframe=data, source_text=\"input\", target_text=\"output\")\n",
        "except Exception as e:\n",
        "    # Handle other unexpected errors\n",
        "    logger.error(\"An unexpected error occurred during fine-tuning:\")\n",
        "    logger.error(traceback.extract_stack())\n",
        "    # Save the model and any relevant data before exiting gracefully\n",
        "    save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Instantiate Target Model for Manual Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual testing target model\n",
        "\n",
        "model_name = input(\"What model from the local models directory would you like to use?\")\n",
        "\n",
        "input_model_directory_path = f\"../models/{model_name}/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Fine Tuned Model Manual Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T5 Variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual test scripts\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "from scripts.file_utils import load_jsonl_data\n",
        "from scripts.constants import bullet_prompt_prefix\n",
        "\n",
        "# Model generation parameter control object\n",
        "model_params = {\n",
        "    # Name of the pre-trained model that will be fine-tuned\n",
        "    \"MODEL\": f\"{input_model_directory_path}\",\n",
        "    # Maximum number of tokens from source text that model accepts\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": max_input_token_length,\n",
        "    # Maximum number of tokens from target text that model generates\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": max_output_token_length,\n",
        "    # Number of alternative sequences generated at each step\n",
        "    # More beams improve results, but increase computation\n",
        "    \"NUM_BEAMS\": number_of_beams,\n",
        "    # Scales logits before soft-max to control randomness\n",
        "    # Lower values (~0) make output more deterministic\n",
        "    \"TEMPERATURE\": 0.1,\n",
        "    # Limits generated tokens to top K probabilities\n",
        "    # Reduces chances of rare word predictions\n",
        "    \"TOP_K\": 50,\n",
        "    # Applies nucleus sampling, limiting token selection to a cumulative probability\n",
        "    # Creates a balance between randomness and determinism\n",
        "    \"TOP_P\": 0.90,\n",
        "}\n",
        "\n",
        "\n",
        "# Load the T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\n",
        "    model_params[\"MODEL\"], model_max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"]\n",
        ")\n",
        "\n",
        "# Load the data from the manual test file\n",
        "data = load_jsonl_data(\n",
        "    \"../data/training/manual_test_set.jsonl\", bullet_prompt_prefix, isDataFrame=False\n",
        ")\n",
        "for line in data:\n",
        "    # Preprocess input\n",
        "    input_text = line[\"input\"]\n",
        "    expected_summary = line[\"output\"]\n",
        "\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        input_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
        "    )\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        num_beams=model_params[\"NUM_BEAMS\"],\n",
        "        temperature=model_params[\"TEMPERATURE\"],\n",
        "        top_k=model_params[\"TOP_K\"],\n",
        "        top_p=model_params[\"TOP_P\"],\n",
        "        early_stopping=True,\n",
        "    )\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"> INPUT TEXT: {input_text}\")\n",
        "    print(f\"\\t> EXPECTED SUMMARY: {expected_summary}\")\n",
        "    print(f\"\\t> GENERATED SUMMARY: {summary}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+sqU2Hgca8RM/Wjv+9kvQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "T5 Fine tuning with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
