{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fine Tuning Training Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Flow\n",
    "\n",
    "- This notebook is used for generating the JSONL files used to fine-tune a model.\n",
    "- The goal is to use [bullet_scraper.py](./scripts/bullet_scraper.py) to scrape for open-source bullets at a particular URL and use those as expected model completions.\n",
    "- These expected model completions have inputs that are generated using through the use of another model's generative capabilities, expanding the acronyms and adding context to the bullet in order to mimic natural human language input.\n",
    "- Once the inputs and expected completions are gathered and cleaned, they can be used to train the The Forge's model.\n",
    "\n",
    "The steps for this notebook are as follows:\n",
    "\n",
    "- [Step 1: Import Dependencies](#step-1-import-dependencies)\n",
    "- [Step 2: Run the Bullet Scraper](#step-2-run-the-bullet-scraper)\n",
    "  - Contains optional block to run scraper on specific websites\n",
    "- [Step 3: Clean the Scraped Data](#step-3-clean-the-scraped-data)\n",
    "  - Contains optional block to run cleaner on specific text files\n",
    "- [Step 4: Consolidate the Cleaned Data](#step-4-consolidate-the-cleaned-data)\n",
    "  - Contains optional block to consolidate data to a different location\n",
    "- [Step 5: Generating Inputs for Completions](#step-5-generating-inputs-for-completions)\n",
    "- [Step 6: Training Inputs from a Fine-Tuned Model](#step-6-training-inputs-from-a-fine-tuned-model)\n",
    "  - Contains optional block to perform extra LLM-enhanced data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Dependencies\n",
    "\n",
    "This block must be run prior to performing any of the steps following this one.\n",
    "\n",
    "This block imports all the necessary scripts and dependencies used by all of the following blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bullet_scraper import *\n",
    "from scripts.file_utils import *\n",
    "from scripts.constants import *\n",
    "from scripts.model_instantiation import *\n",
    "from scripts.data_consolidator import consolidate_files\n",
    "from scripts.prompt import data_modification_prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the Bullet Scraper\n",
    "\n",
    "**_IMPORTANT NOTE:_** The scripts in here are all rudimentary, so stopping and restarting the Jupyter Notebook and kernel may be required to exit the asynchronous scraping processes. Some websites may run much longer than others, while others may block the usage of this scraper completely.\n",
    "\n",
    "This step retrieves \"expected outputs\", bullets, from a variety of sources in order to form the foundation of the training data required to generate the Bullet Forge.\n",
    "\n",
    "Please see the _scripts/_ directory for more details. Please be warned that the scraper, although parallelized, may take a long time to run through all of the identified bullet repositories. The block below runs the scraper on all the easily scrape-able websites located within the [websites.txt](../resources/websites.txt) file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/websites.txt\", \"r\") as file:\n",
    "    urls = [url.strip() for url in file.readlines()]\n",
    "\n",
    "for url in urls:\n",
    "    await bullet_scraper(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to run the scraper one websites at a time. This is useful for the website(s) below, where there is a hard-coded records limit. The block requires the following input:\n",
    "\n",
    "- base_url: The base URL including the protocol, e.g., https://www.afeprbullets.com/results.php?Submit5=Search&strength=Positive&rec=8124\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = input(\"What base URL would you like to start scraping from?\")\n",
    "\n",
    "await bullet_scraper(base_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Scraped Data\n",
    "\n",
    "In this step, the scraped data is given some extra cleaning. This removes bullets that are clearly way too long or short, and provides proper spacing and formatting to bullets that do not follow the standard bullet format.\n",
    "\n",
    "The block below does not require any inputs; however, please note that the input/output of the data cleaning is within the variable `directory_path` below unless it is explicitly changed in the code block below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"../data/raw/\"\n",
    "\n",
    "batch_clean_files(directory_path, bullet_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a specific file to clean. The block below requires the following input:\n",
    "\n",
    "- dirty*file: The name of the text file you want cleaned, from the \\_data/raw/* directory, e.g., `afeprbullets`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory_path = \"../data/raw/\"\n",
    "dirty_file_path = base_directory_path + input(\n",
    "    \"What is the filename of the file you want to clean?\"\n",
    ")\n",
    "\n",
    "clean_file(dirty_file_path, bullet_pattern)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Consolidate the Cleaned Data\n",
    "\n",
    "In this step, all of the expected completions are consolidated into one JSONL file for easier handling in later steps and notebooks. The JSON objects within the JSONL will carry the following structure:\n",
    "\n",
    "```json\n",
    "{ \"input\": \"<DETAIL>\", \"output\": \"<BULLET>\" }\n",
    "```\n",
    "\n",
    "The `output` key stores the expected completion and the `input` key stores the prompt. `<DETAIL>` is the location of the expected inputs a user might provide to the Bullet Forge. [Step 4](#step-4-generating-inputs-for-completions) talks more about the generation of the entire `input` value for training the Bullet Forge.\n",
    "\n",
    "The block below does not require any inputs; however, please note that the output of the consolidated data is within the variable `directory_path` below, and the name of the output file will always be `raw_consolidated_set.jsonl`, unless it is explicitly changed in the code block below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../data/raw/\"\n",
    "output_path = \"../data/raw/raw_consolidated_set.jsonl\"\n",
    "\n",
    "await consolidate_files(base_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a new or existing file to be consolidated to a different file, and not the user's existing, master copy of `raw_consolidated_set.jsonl`.\n",
    "\n",
    "- filename: The name of the file to be consolidated and formatted, e.g. `contributed`. Will also be used as the filename of the output jsonl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input(\"What is the file you would like to consolidate?\")\n",
    "\n",
    "base_path = f\"../data/raw/{filename}\"\n",
    "\n",
    "file_path = f\"{base_path}.txt\"\n",
    "output_path = f\"{base_path}.jsonl\"\n",
    "\n",
    "await consolidate_files(file_path, output_file_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generating Inputs for Completions\n",
    "\n",
    "As discussed in [Step 3](#step-3-consolidate-the-cleaned-data), the bullet scraping, cleaning, and consolidating do not yield the inputs that an actual user of The Forge may provide. The `<DETAIL>` is purposefully left within each `input` so that the JSONL's JSON objects can be fed directly into a prompt-engineered model that can expand the bullet back into natural human language.\n",
    "\n",
    "Below is an example of a completed JSON object for training. Please note that the JSON object below is pretty-formatted for ease of viewing, but in reality, the JSONL will be flat.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": \"As the Subject Matter Expert (SME) for the Exceptional Family Member Program (EFMP), I expertly directed 57 enrollments, handled 194 incoming inquiries, and addressed 101 outgoing inquiries. Through my leadership and efficient processing, we beat the package processing time by 50% and achieved an on-time rate of 99%, significantly improving support for our EFMP families.\",\n",
    "  \"output\": \"- EFMP SME; dir'd 57 enrollments/194 incoming/101 outgoing inquiries--beat pkg processing time 50%/on-time rt 99%\"\n",
    "}\n",
    "```\n",
    "\n",
    "Any model or tool can be used to get the input-output JSON object. For example, you can prompt-engineer ChatGPT using the below:\n",
    "\n",
    "```\n",
    "INSTRUCTIONS: Expand upon condensed information that follows the bullet format `-[ACTION];[IMPACT]--[OUTCOME]`. Within areas of `<DETAIL>` the task is to take the bullet after \"output\" and:\n",
    "1. expand all acronyms or non-standard english words into their original forms,\n",
    "2. provide more context and language to generate full-form sentences that amount to a small paragraph describing the bullet,\n",
    "3. generate the context and language as if the perspective were from that of a member of the military,\n",
    "4. replace the `<DETAIL>` area in the JSONL JSON-object\n",
    "\n",
    "EXAMPLE OUTPUT BELOW:\n",
    "\n",
    "MY PROMPT: `{\"input\": \"<DETAIL>\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`\n",
    "\n",
    "YOUR RESPONSE: `{\"input\": \"As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`.\n",
    "\n",
    "IMPORTANT NOTE: Take the expansions and replace the `<DETAIL>` area in the JSON. If provided multiple JSONS with `<DETAIL>` areas, then iterate through them performing the expansion. Return all of your results in a code block of type JSON, and have each expansion stays on 1 line of the overall code block.\n",
    "\n",
    "ACTION: Please provide an acknowledgement and summary of the instructions above if you understand the upcoming task.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Inputs from a Fine-Tuned Model\n",
    "\n",
    "**_IMPORTANT NOTE:_** Please see the [fine tuning notebook](../notebooks/fine_tuning.ipynb) for steps and details on how to fine tune a model for creating the Bullet Forge's training data.\n",
    "\n",
    "Once enough input-output pairs have been created using the [Step 5](#step-5-training-inputs-from-a-fine-tuned-model) method (a few hundred), the user can then use those completed training pairs to train a model to continue the process of expanding upon the other pairs that are still missing inputs. This involves fine-tuning a model to perform the fill-ins required on `\"input\": <DETAIL>`. Please see the \"**_IMPORTANT NOTE_**\" at the top of this block for this fine tuning.\n",
    "\n",
    "Please note that the output of the data is within the variable `output_path` below, and the name of the output file will always be `raw_bullet_training_set.jsonl`, unless it is explicitly changed in the code block below.\n",
    "\n",
    "The number of lines of JSON data to be processed is capped at 1500. If the script is to be run until the end of the target file or until a different number of lines of JSON data are generated, then it must be explicitly changed in the code block below by adding the following argument in the `data_modification_prompt` function: `stop_at=<NUMBER OF LINES>`. Additionally, the function can be targeted to do one of the following based on the optional `modify_input` value:\n",
    "\n",
    "1. `True`: Model takes in the input and performs a transformation on it based on the provided prompt, placing the result in the input key-value pair\n",
    "2. `False` (DEFAULT): Model takes in the output and performs a transformation on it based on the provided prompt, placing the result in the input key-value pair\n",
    "\n",
    "The block requires the following user input:\n",
    "\n",
    "- model_path: The model's directory or Hugging Face repository, e.g., google/flan-t5-xl, ../models/opera-bullet-interpreter\n",
    "- tokenizer_path: The tokenizer's directory or Hugging Face repository, e.g., google/flan-t5-xl, ../models/opera-bullet-interpreter\n",
    "- save_model_decision: Whether the user wants to save a copy of the model to the local directory, \"yes\" or \"no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filepath = \"../data/raw/raw_consolidated_set.jsonl\"\n",
    "output_filepath = \"../data/raw/raw_bullet_training_set.jsonl\"\n",
    "prompt_prefix = bullet_data_creation_prefix\n",
    "\n",
    "data_modification_prompt(output_filepath, input_filepath, prompt_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional block below performs more data transformation, at the discretion of the user, using a user-selected model and prompt. For example, this could be a focused prompt for fixing grammar and punctuation of the sentences.\n",
    "\n",
    "Beyond the same requirements as the previous block's required user input, the following are also required:\n",
    "\n",
    "- input_filepath: The input file's relative directory path, e.g. ../data/raw/raw_bullet_training_set.jsonl\n",
    "- output_filepath: The output file's relative directory path, e.g. ../data/raw/raw_bullet_training_set_NEW.jsonl\n",
    "- prompt_prefix: The action to be performed by the model on each line of input data, e.g., Fix the grammar, punctuation and spelling, and add more context to the following United States Air and Space Force achievement statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filepath = input(\"What is the relative path to the file for modification?\")\n",
    "output_filepath = input(\"What is the relative path for the output file, including filename?\")\n",
    "prompt_prefix = input(\"What is the action to be performed on the data?\") +  \": \"\n",
    "\n",
    "data_modification_prompt(output_filepath, input_filepath, prompt_prefix, modify_input=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
