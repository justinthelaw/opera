{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fine Tuning Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Flow\n",
    "\n",
    "- This notebook is used for generating the JSONL files used to fine-tune a model. \n",
    "- The goal is to use [bullet_scraper.py](./scripts/bullet_scraper.py) to scrape for open-source bullets at a particular URL and use those as expected model completions. \n",
    "- These expected model completions have inputs that are generated using through the use of another model's generative capabilities, expanding the acronyms and adding context to the bullet in order to mimic natural human language input. \n",
    "- Once the inputs and expected completions are gathered and cleaned, they can be used to train the The Forge's model.\n",
    "\n",
    "The steps for this notebook are as follows:\n",
    "- [Step 1: Run the Bullet Scraper](#step-1-run-the-bullet-scraper)\n",
    "    - Contains optional block to run scraper on specific websites\n",
    "- [Step 2: Clean the Scraped Data](#step-2-clean-the-scraped-data)\n",
    "    - Contains optional block to run cleaner on specific text files\n",
    "- [Step 3: Consolidate the Cleaned Data](#step-3-consolidate-the-cleaned-data)\n",
    "    - Contains optional block to consolidate data to a different location\n",
    "- [Step 4: Generating Inputs for Completions](#step-4-generating-inputs-for-completions)\n",
    "- [Step 5: Training Inputs from a Fine-Tuned Model](#step-5-training-inputs-from-a-fine-tuned-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run the Bullet Scraper\n",
    "\n",
    "**_IMPORTANT NOTE:_** The scripts in here are all rudimentary, so stopping and restarting the Jupyter Notebook and kernel may be required to exit the asynchronous scraping processes. Some websites may run much longer than others, while others may block the usage of this scraper completely.\n",
    "\n",
    "This step retrieves \"expected outputs\", bullets, from a variety of sources in order to form the foundation of the training data required to generate the Bullet Forge.\n",
    "\n",
    "Please see the _scripts/_ directory for more details. Please be warned that the scraper, although parallelized, may take a long time to run through all of the identified bullet repositories. The block below runs the scraper on all the easily scrape-able websites located within the [websites.txt](../resources/websites.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bullet_scraper import bullet_scraper\n",
    "\n",
    "with open('../resources/websites.txt', 'r') as file:\n",
    "    urls = [url.strip() for url in file.readlines()]\n",
    "\n",
    "for url in urls:\n",
    "    await bullet_scraper(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to run the scraper one websites at a time. This is useful for the website(s) below, where there is a hard-coded records limit. The block requires the following input:\n",
    "\n",
    "- base_url: The base URL including the protocol, e.g., https://www.afeprbullets.com/results.php?Submit5=Search&strength=Positive&rec=8124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bullet_scraper import bullet_scraper\n",
    "\n",
    "base_url = input(\"What base URL would you like to start scraping from?\")\n",
    "\n",
    "await bullet_scraper(base_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Scraped Data\n",
    "\n",
    "In this step, the scraped data is given some extra cleaning. This removes bullets that are clearly way too long or short, and provides proper spacing and formatting to bullets that do not follow the standard bullet format.\n",
    "\n",
    "The block below does not require any inputs; however, please note that the input/output of the data cleaning is within the variable `directory_path` below unless it is explicitly changed in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "from scripts.file_utils import batch_clean_files\n",
    "from scripts.constants import bullet_pattern\n",
    "\n",
    "directory_path = \"../data/raw/\"\n",
    "\n",
    "logger.info(f\"Performing extra cleaning on files in directory: {directory_path}\")\n",
    "batch_clean_files(directory_path, bullet_pattern)\n",
    "logger.success(\"Extra cleaning on directory complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a specific file to clean. The block below requires the following input:\n",
    "\n",
    "- dirty_file: The name of the text file you want cleaned, from the _data/raw/_ directory, e.g., `afeprbullets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "from scripts.file_utils import clean_file\n",
    "from scripts.constants import bullet_pattern\n",
    "\n",
    "base_directory_path = \"../data/raw/\"\n",
    "dirty_file_path = base_directory_path + input(\n",
    "    \"What is the filename of the file you want to clean?\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Performing extra cleaning on file: {dirty_file_path}\")\n",
    "clean_file(dirty_file_path, bullet_pattern)\n",
    "logger.success(\"Extra cleaning on file complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Consolidate the Cleaned Data\n",
    "\n",
    "In this step, all of the expected completions are consolidated into one JSONL file for easier handling in later steps and notebooks. The JSON objects within the JSONL will carry the following structure: \n",
    "\n",
    "```json\n",
    "{\"input\": \"<ADD DETAIL>\", \"output\": \"<THE SCRAPED, CLEANED BULLET>\"}\n",
    "```\n",
    "\n",
    "The `output` key stores the expected completion and the `input` key stores the prompt. `<ADD DETAIL>` is the location of the expexted inputs a user might provide to the Bullet Forge. [Step 4](#step-4-generating-inputs-for-completions) talks more about the generation of the entire `input` value for training the Bullet Forge.\n",
    "\n",
    "The block below does not require any inputs; however, please note that the output of the consolidated data is within the variable `directory_path` below, and the name of the output file will always be `raw_consolidated_set.jsonl`, unless it is explicitly changed in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_consolidator import consolidate_files\n",
    "\n",
    "base_path = \"../data/raw/\"\n",
    "output_path = \"../data/raw/raw_consolidated_set.jsonl\"\n",
    "\n",
    "await consolidate_files(base_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a new or existing file to be consolidated to a different file, and not the user's existing, master copy of `raw_consolidated_set.jsonl`.\n",
    "- filename: The name of the file to be consolidated and formatted, e.g. `contributed`. Will also be used as the filename of the output jsonl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_consolidator import consolidate_files\n",
    "\n",
    "filename = input(\"What is the file you would like to consolidate?\")\n",
    "\n",
    "base_path = f\"../data/raw/{filename}\"\n",
    "\n",
    "file_path = f\"{base_path}.txt\"\n",
    "output_path = f\"{base_path}.jsonl\"\n",
    "\n",
    "await consolidate_files(file_path, output_file_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generating Inputs for Completions\n",
    "\n",
    "As discussed in [Step 3](#step-3-consolidate-the-cleaned-data), the bullet scraping, cleaning, and consolidating do not yield the inputs that an actual user of The Forge may provide. The `<ADD DETAIL>` is purposefully left within each `input` so that the JSONL's JSON objects can be fed directly into a prompt-engineered model that can expand the bullet back into natural human language. \n",
    "\n",
    "Below is an example of a completed JSON object for training. Please note that the JSON object below is pretty-formatted for ease of viewing, but in reality, the JSONL will be flat.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"input\": \"As the Subject Matter Expert (SME) for the Exceptional Family Member Program (EFMP), I expertly directed 57 enrollments, handled 194 incoming inquiries, and addressed 101 outgoing inquiries. Through my leadership and efficient processing, we beat the package processing time by 50% and achieved an on-time rate of 99%, significantly improving support for our EFMP families.\",\n",
    "    \"output\": \"- EFMP SME; dir'd 57 enrollments/194 incoming/101 outgoing inquiries--beat pkg processing time 50%/on-time rt 99%\"\n",
    "}\n",
    "```\n",
    "\n",
    "Any model or tool can be used to get the input-output JSON object. For example, you can prompt-engineer ChatGPT using the below:\n",
    "\n",
    "```\n",
    "INSTRUCTIONS: Expand upon condensed information that follows the bullet format `-[ACTION];[IMPACT]--[OUTCOME]`. Within areas of `<ADD DETAIL>` the task is to take the bullet after \"output\" and:\n",
    "1. expand all acronyms or non-standard english words into their original forms, \n",
    "2. provide more context and language to generate full-form sentences that amount to a small paragraph describing the bullet, \n",
    "3. generate the context and language as if the perspective were from that of a member of the military, \n",
    "4. replace the `<ADD DETAIL>` area in the JSONL JSON-object\n",
    "\n",
    "EXAMPLE OUTPUT BELOW:\n",
    "\n",
    "MY PROMPT: `{\"input\": \"<ADD DETAIL>\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`\n",
    "\n",
    "YOUR RESPONSE: `{\"input\": \"As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`. \n",
    "\n",
    "IMPORTANT NOTE: Take the expansions and replace the `<ADD DETAIL>` area in the JSON. If provided multiple JSONS with `<ADD DETAIL>` areas, then iterate through them performing the expansion. Return all of your results in a code block of type JSON, and have each expansion stays on 1 line of the overall code block. \n",
    "\n",
    "ACTION: Please provide an acknowledgement and summary of the instructions above if you understand the upcoming task.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Inputs from a Fine-Tuned Model\n",
    "\n",
    "**_IMPORTANT NOTE:_** Please see the [fine tuning notebook](../notebooks/fine_tuning.ipynb) for steps and details on how to fine tune a model for creating the Bullet Forge's training data.\n",
    "\n",
    "Once enough input-output pairs have been created using the [Step 5](#step-5-training-inputs-from-a-fine-tuned-model) method (a few hundred), the user can then use those completed training pairs to train a model to continue the process of expanding upon the other 30K pairs that are still missing inputs. This involves fine-tuning a model to perform the fill-ins required on `\"input\": <ADD DETAIL>`. Please see the \"**_IMPORTANT NOTE_**\" at the top of this block for this fine tuning.\n",
    "\n",
    "The block below inferences a fine-tuned model trained to create new inputs from bullets using the existing [training and validation set](../data/training/training_validation_set.jsonl). The block below requires the following inputs:\n",
    "\n",
    "- model_path: the relative directory path or Hugging Face repository that holds all of the model files necessary for instantiation and inferencing\n",
    "- tokenizer_path: the relative directory path or Hugging Face repository that holds the correct tokenizer for encoding and decoding inputs and outputs\n",
    "\n",
    "Please note that the output of the data is within the variable `output_path` below, and the name of the output file will always be `training_validation_set.jsonl`, unless it is explicitly changed in the code block below. Additionally, the number of lines of JSON data is capped at 1500 using the `maximum_lines_of_data` variable. If the script is to be run until the end of the target file or until a different number of lines of JSON data are generated, then it must be explicitly changed in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from loguru import logger\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "from scripts.constants import bullet_data_creation_prefix\n",
    "from scripts.file_utils import load_jsonl_data, append_line_to_file\n",
    "\n",
    "# Path of the output file\n",
    "output_filepath = \"../data/raw/raw_bullet_training_set.jsonl\"\n",
    "# Max length of tokens a user may enter for summarization\n",
    "# Increasing this beyond 512 may increase compute time significantly\n",
    "max_input_token_length = 512\n",
    "# Max length of tokens the model should output for the summary\n",
    "# Approximately the number of tokens it may take to generate a bullet\n",
    "max_output_token_length = 512\n",
    "# Beams to use for beam search algorithm\n",
    "# Increased beams means increased quality, but increased compute time\n",
    "number_of_beams = 6\n",
    "# Scales logits before soft-max to control randomness\n",
    "# Lower values (~0) make output more deterministic\n",
    "temperature = 0.5\n",
    "# Limits generated tokens to top K probabilities\n",
    "# Reduces chances of rare word predictions\n",
    "top_k = 50\n",
    "# Applies nucleus sampling, limiting token selection to a cumulative probability\n",
    "# Creates a balance between randomness and determinism\n",
    "top_p = 0.90\n",
    "\n",
    "try:\n",
    "    # Path of the pre-trained model that will be used\n",
    "    model_path = input(\n",
    "        \"Input a checkpoint model's Hugging Face repository or a relative path\"\n",
    "    )\n",
    "    # Path of the pre-trained model tokenizer that will be used\n",
    "    # Must match the model checkpoint's signature\n",
    "    tokenizer_path = input(\n",
    "        \"Input a tokenizer's Hugging Face repository or a relative path\"\n",
    "    )\n",
    "    # Load the pre-trained model and tokenizer\n",
    "    logger.info(\n",
    "        f\"Instantiating tokenizer from {tokenizer_path}, and model from {model_path}\"\n",
    "    )\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\n",
    "        f\"{tokenizer_path}\",\n",
    "        model_max_length=max_input_token_length,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    input_model = T5ForConditionalGeneration.from_pretrained(f\"{model_path}\")\n",
    "    logger.info(f\"Loading {model_path}...\")\n",
    "    # Set device to be used based on GPU availability\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Model is sent to device for use\n",
    "    model = input_model.to(device)  # type: ignore\n",
    "    logger.success(\"Instantiated target tokenizer and model\")\n",
    "\n",
    "    # Preprocess input\n",
    "    inputs_array = load_jsonl_data(\"../data/raw/raw_consolidated_set.jsonl\")\n",
    "    \n",
    "    maximum_lines_of_data = 1500\n",
    "\n",
    "    for count, input_line in enumerate(inputs_array):\n",
    "        if count == (maximum_lines_of_data - 1):\n",
    "            raise InterruptedError()\n",
    "\n",
    "        input_text = bullet_data_creation_prefix + input_line[\"output\"]\n",
    "\n",
    "        encoded_input_text = tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_input_token_length,\n",
    "        )\n",
    "\n",
    "        # Generate summary\n",
    "        summary_ids = model.generate(\n",
    "            encoded_input_text[\"input_ids\"],\n",
    "            attention_mask=encoded_input_text[\"attention_mask\"],\n",
    "            max_length=max_output_token_length,\n",
    "            num_beams=number_of_beams,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "        output_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # input_text and output_text insert into data sets\n",
    "        line_of_data = (\n",
    "            f'{{\"input\": \"{output_text}\", \"output\": \"{input_line[\"output\"]}\"}}'\n",
    "        )\n",
    "        append_line_to_file(output_filepath, line_of_data)\n",
    "        logger.success(f\"Appended {count + 1}/{maximum_lines_of_data}: {line_of_data}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning(\"Received interrupt, stopping script...\")\n",
    "except InterruptedError:\n",
    "    logger.success(\"Data generation complete! Stopping script...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during generation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional block below performs an extra prompt, at the discretion of the user, on the data that was generated from the above step. This could be, for example, a focused prompt for fixing grammar, adding context, and/or taking in an acronyms list and ensuring the accuracy of the expanded acronyms for each input-output pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: (optional) creation of another prompting step to be performed on the data generated by the step prior to this one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
