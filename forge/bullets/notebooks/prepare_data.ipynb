{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fine Tuning Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Flow\n",
    "\n",
    "- This notebook is used for generating the JSONL files used to fine-tune a model. \n",
    "- The goal is to use [bullet_scraper.py](./scripts/bullet_scraper.py) to scrape for open-source bullets at a particular URL and use those as expected model completions. \n",
    "- These expected model completions have inputs that are generated using through the use of another model's generative capabilities, expanding the acronyms and adding context to the bullet in order to mimic natural human language input. \n",
    "- Once the inputs and expected completions are gathered and cleaned, they can be used to train the The Forge's model.\n",
    "\n",
    "The steps for this notebook are as follows:\n",
    "- [Step 1: Run the Bullet Scraper](#step-1-run-the-bullet-scraper)\n",
    "    - Contains optional block to run scraper on specific websites\n",
    "- [Step 2: Clean the Scraped Data](#step-2-clean-the-scraped-data)\n",
    "    - Contains optional block to run cleaner on specific text files\n",
    "- [Step 3: Consolidate the Cleaned Data](#step-3-consolidate-the-cleaned-data)\n",
    "    - Contains optional block to consolidate data to a different location\n",
    "- [Step 4: Generating Inputs for Completions](#step-4-generating-inputs-for-completions)\n",
    "- [Step 5: Training Inputs from a Fine-Tuned Model](#step-5-training-inputs-from-a-fine-tuned-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run the Bullet Scraper\n",
    "\n",
    "**_IMPORTANT NOTE:_** The scripts in here are all rudimentary, so stopping and restarting the Jupyter Notebook and kernel may be required to exit the asynchronous scraping processes. Some websites may run much longer than others, while others may block the usage of this scraper completely.\n",
    "\n",
    "This step retrieves \"expected outputs\", bullets, from a variety of sources in order to form the foundation of the training data required to generate the Bullet Forge.\n",
    "\n",
    "Please see the _scripts/_ directory for more details. Please be warned that the scraper, although parallelized, may take a long time to run through all of the identified bullet repositories. The block below runs the scraper on all the easily scrape-able websites located within the [websites.txt](../resources/websites.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bullet_scraper import bullet_scraper\n",
    "\n",
    "with open('../resources/websites.txt', 'r') as file:\n",
    "    urls = [url.strip() for url in file.readlines()]\n",
    "\n",
    "for url in urls:\n",
    "    await bullet_scraper(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to run the scraper one websites at a time. This is useful for the website(s) below, where there is a hard-coded records limit. The block requires the following input:\n",
    "\n",
    "- base_url: The base URL including the protocol, e.g., https://www.afeprbullets.com/results.php?Submit5=Search&strength=Positive&rec=8124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bullet_scraper import bullet_scraper\n",
    "\n",
    "base_url = input(\"What base URL would you like to start scraping from?\")\n",
    "\n",
    "await bullet_scraper(base_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Scraped Data\n",
    "\n",
    "In this step, the scraped data is given some extra cleaning. This removes bullets that are clearly way too long or short, and provides proper spacing and formatting to bullets that do not follow the standard bullet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "from scripts.file_utils import batch_clean_files\n",
    "from scripts.constants import bullet_pattern\n",
    "\n",
    "base_directory_path = \"../data/raw/\"\n",
    "\n",
    "logger.info(f\"Performing extra cleaning on files in directory: {base_directory_path}\")\n",
    "batch_clean_files(base_directory_path, bullet_pattern)\n",
    "logger.success(\"Extra cleaning on directory complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a specific file to clean. The block below requires the following input:\n",
    "\n",
    "- dirty_file: The name of the text file you want cleaned, from the _data/raw/_ directory, e.g., `afeprbullets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "from scripts.file_utils import clean_file\n",
    "from scripts.constants import bullet_pattern\n",
    "\n",
    "base_directory_path = \"../data/raw/\"\n",
    "dirty_file_path = base_directory_path + input(\n",
    "    \"What is the filename of the file you want to clean?\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Performing extra cleaning on file: {dirty_file_path}\")\n",
    "clean_file(dirty_file_path, bullet_pattern)\n",
    "logger.success(\"Extra cleaning on file complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Consolidate the Cleaned Data\n",
    "\n",
    "In this step, all of the expected completions are consolidated into one JSONL file for easier handling in later steps and notebooks. The JSON objects within the JSONL will carry the following structure: \n",
    "\n",
    "```json\n",
    "{\"input\": \"<ADD DETAIL>\", \"output\": \"<THE SCRAPED, CLEANED BULLET>\"}\n",
    "```\n",
    "\n",
    "The `output` key stores the expected completion and the `input` key stores the prompt. `<ADD DETAIL>` is the location of the expexted inputs a user might provide to the Bullet Forge. [Step 4](#step-4-generating-inputs-for-completions) talks more about the generation of the entire `input` value for training the Bullet Forge.\n",
    "\n",
    "The block below does not require any inputs; however, please note that the output of the consolidated data is within the variable `directory_path` below, and the name of the output file will always be `consolidated_set.jsonl`, unless it is explicitly changed in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from scripts.data_consolidator import consolidate_files\n",
    "\n",
    "base_directory_path = \"../data/raw/\"\n",
    "output_directory_path = \"../data/raw/consolidated_set.jsonl\"\n",
    "\n",
    "file_paths = \" \".join([os.path.join(root, file) for root, _, files in os.walk(base_directory_path) for file in files])\n",
    "\n",
    "await consolidate_files(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a new or existing file to be consolidated to a different file, and not the user's existing, master copy of `consolidated_set.jsonl`.\n",
    "- filename: The name of the file to be consolidated and formatted, e.g. `contributed`. Will also be used as the filename of the output jsonl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_consolidator import consolidate_files\n",
    "\n",
    "filename = input(\"What is the file you would like to consolidate?\")\n",
    "\n",
    "base_directory_path = f\"../data/raw/{filename}\"\n",
    "\n",
    "file_path = f\"{base_directory_path}.txt\"\n",
    "output_path = f\"{base_directory_path}.jsonl\"\n",
    "\n",
    "await consolidate_files(file_path, output_file_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generating Inputs for Completions\n",
    "\n",
    "As discussed in [Step 3](#step-3-consolidate-the-cleaned-data), the bullet scraping, cleaning, and consolidating do not yield the inputs that an actual user of The Forge may provide. The `<ADD DETAIL>` is purposefully left within each `input` so that the JSONL's JSON objects can be fed directly into a prompt-engineered model that can expand the bullet back into natural human language. \n",
    "\n",
    "Below is an example of a completed JSON object for training. Please note that the JSON object below is pretty-formatted for ease of viewing, but in reality, the JSONL will be flat.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"input\": \"As the Subject Matter Expert (SME) for the Exceptional Family Member Program (EFMP), I expertly directed 57 enrollments, handled 194 incoming inquiries, and addressed 101 outgoing inquiries. Through my leadership and efficient processing, we beat the package processing time by 50% and achieved an on-time rate of 99%, significantly improving support for our EFMP families.\",\n",
    "    \"output\": \"- EFMP SME; dir'd 57 enrollments/194 incoming/101 outgoing inquiries--beat pkg processing time 50%/on-time rt 99%\"\n",
    "}\n",
    "```\n",
    "\n",
    "Any model or tool can be used to get the input-output JSON object. For example, you can prompt-engineer ChatGPT using the below:\n",
    "\n",
    "```\n",
    "INSTRUCTIONS: Expand upon condensed information that follows the bullet format `-[ACTION];[IMPACT]--[OUTCOME]`. Within areas of `<ADD DETAIL>` the task is to take the bullet after \"output\" and:\n",
    "1. expand all acronyms or non-standard english words into their original forms, \n",
    "2. provide more context and language to generate full-form sentences that amount to a small paragraph describing the bullet, \n",
    "3. generate the context and language as if the perspective were from that of a member of the military, \n",
    "4. replace the `<ADD DETAIL>` area in the JSONL JSON-object\n",
    "\n",
    "EXAMPLE OUTPUT BELOW:\n",
    "\n",
    "MY PROMPT: `{\"input\": \"<ADD DETAIL>\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`\n",
    "\n",
    "YOUR RESPONSE: `{\"input\": \"As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`. \n",
    "\n",
    "IMPORTANT NOTE: Take the expansions and replace the `<ADD DETAIL>` area in the JSON. If provided multiple JSONS with `<ADD DETAIL>` areas, then iterate through them performing the expansion. Return all of your results in a code block of type JSON, and have each expansion stays on 1 line of the overall code block. \n",
    "\n",
    "ACTION: Please provide an acknowledgement and summary of the instructions above if you understand the upcoming task.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Inputs from a Fine-Tuned Model\n",
    "\n",
    "**_IMPORTANT NOTE:_** Please see the [fine tuning notebook](../notebooks/fine_tuning.ipynb) for steps and details on how to fine tune a model for creating the Bullet Forge's training data.\n",
    "\n",
    "Once enough input-output pairs have been created using the [Step 5](#step-5-training-inputs-from-a-fine-tuned-model) method (a few hundred), the user can then use those completed training pairs to train a model to continue the process of expanding upon the other 30K pairs that are still missing inputs. This involves fine-tuning a model to perform the fill-ins required on `\"input\": <ADD DETAIL>`. Please see the \"**_IMPORTANT NOTE_**\" at the top of this block for this fine tuning.\n",
    "\n",
    "The block below contains a simple fine-tuning jon on a smaller model to train it to create new inputs from bullets using the existing [training and validation set](../data/training/training_validation_set.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 1 column 94 (char 93)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     30\u001b[0m top_p \u001b[39m=\u001b[39m \u001b[39m0.90\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Load the pre-trained model and tokenizer\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# logger.info(f\"Instantiating tokenizer from {tokenizer_path}, and model from {model_path}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# tokenizer = T5Tokenizer.from_pretrained(f\"{tokenizer_path}\", model_max_length=max_input_token_length, add_special_tokens = False)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[39m# Preprocess input\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m inputs_array \u001b[39m=\u001b[39m load_jsonl_data(\u001b[39m\"\u001b[39;49m\u001b[39m../data/training/consolidated_set.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m, bullet_data_creation_prefix, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m inputs_array:\n\u001b[1;32m     49\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39minput\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/personal/opera/forge/bullets/notebooks/scripts/file_utils.py:136\u001b[0m, in \u001b[0;36mload_jsonl_data\u001b[0;34m(filepath, prefix, isDataFrame)\u001b[0m\n\u001b[1;32m    132\u001b[0m     file_contents \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m    134\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file_contents:\n\u001b[1;32m    135\u001b[0m     \u001b[39m# Each json object is stored as an dict in an array\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(line)\n\u001b[1;32m    137\u001b[0m     jsonl_array\u001b[39m.\u001b[39mappend(data)\n\u001b[1;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m jsonl_array\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 1 column 94 (char 93)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from loguru import logger\n",
    "\n",
    "from scripts.constants import bullet_data_creation_prefix\n",
    "from scripts.file_utils import load_jsonl_data\n",
    "\n",
    "# Path of the pre-trained model that will be used\n",
    "model_path = input(\"Input a checkpoint model's Hugging Face repository or a relative path\")\n",
    "# Path of the pre-trained model tokenizer that will be used\n",
    "# Must match the model checkpoint's signature\n",
    "tokenizer_path = input(\"Input a tokenizer's Hugging Face repository or a relative path\")\n",
    "# Max length of tokens a user may enter for summarization\n",
    "# Increasing this beyond 512 may increase compute time significantly\n",
    "max_input_token_length = 512\n",
    "# Max length of tokens the model should output for the summary\n",
    "# Approximately the number of tokens it may take to generate a bullet\n",
    "max_output_token_length = 512\n",
    "# Beams to use for beam search algorithm\n",
    "# Increased beams means increased quality, but increased compute time\n",
    "number_of_beams = 6\n",
    "# Scales logits before soft-max to control randomness\n",
    "# Lower values (~0) make output more deterministic\n",
    "temperature = 0.5\n",
    "# Limits generated tokens to top K probabilities\n",
    "# Reduces chances of rare word predictions\n",
    "top_k = 50\n",
    "# Applies nucleus sampling, limiting token selection to a cumulative probability\n",
    "# Creates a balance between randomness and determinism\n",
    "top_p = 0.90\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "# logger.info(f\"Instantiating tokenizer from {tokenizer_path}, and model from {model_path}\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(f\"{tokenizer_path}\", model_max_length=max_input_token_length, add_special_tokens = False)\n",
    "# input_model = T5ForConditionalGeneration.from_pretrained(f\"{model_path}\")\n",
    "# logger.info(f\"Loading {model_path}...\")\n",
    "# # Set device to be used based on GPU availability\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # Model is sent to device for use\n",
    "# model = input_model.to(device) # type: ignore\n",
    "# logger.success(\"Instantiated target tokenizer and model\")\n",
    "\n",
    "# Load the data from the manual test file\n",
    "\n",
    "# Preprocess input\n",
    "inputs_array = load_jsonl_data(\"../data/training/consolidated_set.jsonl\", bullet_data_creation_prefix, isDataFrame=False)\n",
    "\n",
    "for input in inputs_array:\n",
    "    print(input)\n",
    "\n",
    "# encoded_input_text = tokenizer.encode_plus(\n",
    "#     input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_token_length\n",
    "# )\n",
    "\n",
    "# # Generate summary\n",
    "# summary_ids = model.generate(\n",
    "#     encoded_input_text[\"input_ids\"],\n",
    "#     attention_mask=encoded_input_text[\"attention_mask\"],\n",
    "#     max_length=max_output_token_length,\n",
    "#     num_beams=number_of_beams,\n",
    "#     temperature=temperature,\n",
    "#     top_k=top_k,\n",
    "#     top_p=top_p,\n",
    "#     early_stopping=True,\n",
    "# )\n",
    "\n",
    "# output_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# input_text and output_text insert into data sets\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
