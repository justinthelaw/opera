{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fine Tuning Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Flow\n",
    "\n",
    "- This notebook is used for generating the JSONL files used to fine-tune a model. \n",
    "- The goal is to use [bullet_scraper.py](./scripts/bullet_scraper.py) to scrape for open-source bullets at a particular URL and use those as expected model completions. \n",
    "- These expected model completions have inputs that are generated using through the use of another model's generative capabilities, expanding the acronyms and adding context to the bullet in order to mimic natural human language input. \n",
    "- Once the inputs and expected completions are gathered and cleaned, they can be used to train the The Forge's model.\n",
    "\n",
    "The steps for this notebook are as follows:\n",
    "- [Step 1: Run the Bullet Scraper](#step-1-run-the-bullet-scraper)\n",
    "- [Step 2: Clean the Scraped Data](#step-2-clean-the-scraped-data)\n",
    "- [Step 3: Consolidate the Cleaned Data](#step-3-consolidate-the-cleaned-data)\n",
    "- [Step 4: Generating Inputs for Completions](#step-4-generating-inputs-for-completions)\n",
    "- [Step 5: Training Inputs from a Fine-Tuned Model](#step-5-training-inputs-from-a-fine-tuned-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run the Bullet Scraper\n",
    "\n",
    "**_IMPORTANT NOTE:_**: The scripts in here are all rudimentary, so stopping and restarting hr Jupyter process and kernel may be required to exit the asynchronous scraping processes. \n",
    "\n",
    "This step retrieves \"expected completions\", bullets, from a variety of sources in order to form the foundation of the training data.\n",
    "\n",
    "Please see the _scripts/_ directory for more details. Please be warned that the scraper, although parallelized, may take a long time to run through all of the identified bullet repositories. The block below runs the scraper on all the easily scrape-able websites located within the [websites.txt](../resources/websites.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bullet_scraper import bullet_scraper\n",
    "\n",
    "with open('../resources/websites.txt', 'r') as file:\n",
    "    urls = [url.strip() for url in file.readlines()]\n",
    "\n",
    "for url in urls:\n",
    "    await bullet_scraper(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to run the scraper one websites at a time. This is useful for the website(s) below, where there is a hard-coded records limit. Please note that this is a very poorly crafted cronjob, so once success has been logged the first few times on a website, wait 30 more seconds and then stop the script. If you don't stop the script manually, you may end up waiting a really long time for the script to shutdown. The block requires the following input:\n",
    "\n",
    "- base_url: The base URL including the protocol, e.g., https://www.afeprbullets.com/results.php?Submit5=Search&strength=Positive&rec=8124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bullet_scraper import bullet_scraper\n",
    "\n",
    "base_url = input(\"What base URL would you like to start scraping from?\")\n",
    "\n",
    "await bullet_scraper(base_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Scraped Data\n",
    "\n",
    "In this step, the scraped data is given some extra cleaning. This removes bullets that are clearly way too long or short, and provides proper spacing and formatting to bullets that do not follow the standard bullet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "from scripts.file_utils import batch_clean_files\n",
    "from scripts.constants import bullet_pattern\n",
    "\n",
    "base_directory_path = \"../data/raw/\"\n",
    "\n",
    "logger.info(f\"Performing extra cleaning on files in directory: {base_directory_path}\")\n",
    "batch_clean_files(base_directory_path, bullet_pattern)\n",
    "logger.success(\"Extra cleaning on directory complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a specific file to clean. The block below requires the following input:\n",
    "\n",
    "- dirty_file: The name of the text file you want cleaned, from the _data/raw/_ directory, e.g., `afeprbullets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "from scripts.file_utils import clean_file\n",
    "from scripts.constants import bullet_pattern\n",
    "\n",
    "base_directory_path = \"../data/raw/\"\n",
    "dirty_file_path = base_directory_path + input(\n",
    "    \"What is the filename of the file you want to clean?\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Performing extra cleaning on file: {dirty_file_path}\")\n",
    "clean_file(dirty_file_path, bullet_pattern)\n",
    "logger.success(\"Extra cleaning on file complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Consolidate the Cleaned Data\n",
    "\n",
    "In this step, all of the expected completions are consolidated into one JSONL file for easier handling in later steps and notebooks. The JSON objects within the JSONL will carry the following structure: \n",
    "\n",
    "```json\n",
    "{\"input\": \"<ADD DETAIL>\", \"output\": \"{line}\"}\n",
    "```\n",
    "\n",
    "The `output` key stores the expected completion and the `input` key stores the prompt. `<ADD DETAIL>` is the location in which the `output` requires the The Forge user's input. [Step 4](#step-4-generating-inputs-for-completions) talks more about the generation of the entire `input` key's value.\n",
    "\n",
    "The block below does not require any inputs; however, do note that the output of the consolidated data is within the variable `directory_path` below, and the name of the output file will always be `consolidated_set.jsonl`, unless the user explicitly changes it within the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from scripts.data_consolidator import consolidate_files\n",
    "\n",
    "base_directory_path = \"../data/raw/\"\n",
    "output_directory_path = \"../data/raw/consolidated_set.jsonl\"\n",
    "\n",
    "file_paths = \" \".join([os.path.join(root, file) for root, _, files in os.walk(base_directory_path) for file in files])\n",
    "\n",
    "await consolidate_files(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a new or existing file to be consolidated to a different file, and not the user's existing, master copy of `consolidated_set.jsonl`.\n",
    "- filename: The name of the file to be consolidated and formatted, e.g. `contributed`\n",
    "- output_filename: The of the file not including the extension, e.g., `contributed_consolidated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_consolidator import consolidate_files\n",
    "\n",
    "filename = input(\"What is the file you would like to consolidate?\")\n",
    "\n",
    "base_directory_path = f\"../data/raw/{filename}\"\n",
    "\n",
    "file_path = f\"{base_directory_path}.txt\"\n",
    "output_path = f\"{base_directory_path}.jsonl\"\n",
    "\n",
    "await consolidate_files(file_path, output_file_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generating Inputs for Completions\n",
    "\n",
    "As discussed in [Step 3](#step-3-consolidate-the-cleaned-data), the bullet scraping, cleaning, and consolidating do not yield the inputs that an actual user of The Forge may provide. The `<ADD DETAIL>` is purposefully left within each `input` so that the JSONL's JSON objects can be fed directly into a prompt-engineered model that can expand the bullet back into natural human language. \n",
    "\n",
    "Below is an example of a completed JSON object for training. Please note that the JSON object below is pretty-formatted for ease of viewing, but in reality, the JSONL will be flat.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"input\": \"As the Subject Matter Expert (SME) for the Exceptional Family Member Program (EFMP), I expertly directed 57 enrollments, handled 194 incoming inquiries, and addressed 101 outgoing inquiries. Through my leadership and efficient processing, we beat the package processing time by 50% and achieved an on-time rate of 99%, significantly improving support for our EFMP families.\",\n",
    "    \"output\": \"- EFMP SME; dir'd 57 enrollments/194 incoming/101 outgoing inquiries--beat pkg processing time 50%/on-time rt 99%\"\n",
    "}\n",
    "```\n",
    "\n",
    "Any model or tool can be used to get the input-output JSON object. For example, you can prompt-engineer ChatGPT using the below:\n",
    "\n",
    "```\n",
    "INSTRUCTIONS: Expand upon condensed information that follows the bullet format `-[ACTION];[IMPACT]--[OUTCOME]`. Within areas of `<ADD DETAIL>` the task is to take the bullet after \"output\" and:\n",
    "1. expand all acronyms or non-standard english words into their original forms, \n",
    "2. provide more context and language to generate full-form sentences that amount to a small paragraph describing the bullet, \n",
    "3. generate the context and language as if the perspective were from that of a member of the military, \n",
    "4. replace the `<ADD DETAIL>` area in the JSONL JSON-object\n",
    "\n",
    "EXAMPLE OUTPUT BELOW:\n",
    "\n",
    "MY PROMPT: `{\"input\": \"<ADD DETAIL>\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`\n",
    "\n",
    "YOUR RESPONSE: `{\"input\": \"As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`. \n",
    "\n",
    "IMPORTANT NOTE: Take the expansions and replace the `<ADD DETAIL>` area in the JSON. If provided multiple JSONS with `<ADD DETAIL>` areas, then iterate through them performing the expansion. Return all of your results in a code block of type JSON, and have each expansion stays on 1 line of the overall code block. \n",
    "\n",
    "ACTION: Please provide an acknowledgement and summary of the instructions above if you understand the upcoming task.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Inputs from a Fine-Tuned Model\n",
    "\n",
    "Once enough input-output pairs have been created using the [Step 5](#step-5-training-inputs-from-a-fine-tuned-model) method (around a few hundred), the user can then use those completed training pairs to continue the process on the other 30K pairs that are still missing inputs for their respective outputs. This involves fine-tuning a model to perform the fill-ins required on `\"input\": <ADD DETAIL>`.\n",
    "\n",
    "The block below contains a simple fine-tuning jon on a smaller model to train it to create new inputs from bullets using the existing [training and validation set](../data/training/training_validation_set.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be created script using free Microsoft employee OpenAI access token..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
