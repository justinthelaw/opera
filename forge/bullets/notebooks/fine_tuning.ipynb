{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Forge Model Fine Tuning: T5x Variants\n",
        "\n",
        "This notebook is specifically geared towards the fine tuning of T5x variant models using PyTorch and Hugging Face Transformers. the notebook contains model fine tuning and monitoring scripts to be used for the generation of the Forge's training data set and the production Forge model.\n",
        "\n",
        "This notebook includes options, through user input, to direct the fine tuning towards training data generation or bullet generation models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Instantiate Global Parameters\n",
        "\n",
        "Thi step must be completed prior to performing any of the steps following this one. These parameters will be used for instantiating, fine tuning, and prompting the model.\n",
        "\n",
        "When selecting a model and tokenizer, ensure the tokenizer is of the same signature and base model as the selected model. For example, if using google/flan-t5-xl as the model then the tokenizer should be google/flan-t5-base.\n",
        "\n",
        "Some T5x variant checkpoints to try out for fine tuning or inferencing include, but are note limited to:\n",
        "- [MBZUAI/LaMini-T5-738M](https://huggingface.co/MBZUAI/LaMini-T5-738M)\n",
        "- [t5-base](https://huggingface.co/t5-base)\n",
        "- [google/t5-efficient-tiny](https://huggingface.co/google/t5-efficient-tiny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path of the pre-trained model that will be used\n",
        "model_path = input(\"Input a checkpoint model's Hugging Face repository or a relative path\")\n",
        "# Path of the pre-trained model tokenizer that will be used\n",
        "# Must match the model checkpoint's signature\n",
        "tokenizer_path = input(\"Input a tokenizer's Hugging Face repository or a relative path\")\n",
        "# Max length of tokens a user may enter for summarization\n",
        "# Increasing this beyond 512 may increase compute time significantly\n",
        "max_input_token_length = 512\n",
        "# Max length of tokens the model should output for the summary\n",
        "# Approximately the number of tokens it may take to generate a bullet\n",
        "max_output_token_length = 512\n",
        "# Beams to use for beam search algorithm\n",
        "# Increased beams means increased quality, but increased compute time\n",
        "number_of_beams = 6\n",
        "# Number of examples per batch during training\n",
        "# Larger batch sizes require more memory, but can speed up training\n",
        "train_batch_size = 1\n",
        "# Number of full passes through the entire training dataset\n",
        "# More epochs can lead to better performance, but risk over-fitting\n",
        "train_epochs = 6\n",
        "# Number of examples per batch during validation\n",
        "# Larger batch sizes require more memory, but can speed up the validation process\n",
        "valid_batch_size = 1\n",
        "# Number of full passes through the entire validation dataset\n",
        "# Typically kept to a single epoch as the validation set does not need to be repeatedly passed\n",
        "val_epochs = 1\n",
        "# Affects how quickly or slowly a model learns\n",
        "# Too high can cause instability, too low can cause slow learning\n",
        "learning_rate = 1e-4\n",
        "# Random seed to ensure reproducibility\n",
        "# Using the same seed will yield the same model given the same data and training process\n",
        "seed = 8\n",
        "# Multiplier to penalize repeated n-grams\n",
        "# Higher values discourage repetition in the generated text\n",
        "repetition_penalty = 1\n",
        "# Penalty applied for producing long sequences\n",
        "# Higher values encourage longer sequences\n",
        "length_penalty = 0\n",
        "# The number of steps to take before the gradient is averaged and applied\n",
        "# Helps in stabilizing training and requires less memory\n",
        "gradient_accumulation_steps = 1\n",
        "# Weight decay introduced to the optimizer to prevent over-fitting\n",
        "# Regularization strategy by adding a small penalty, typically the L2 norm of the weights\n",
        "weight_decay = 0.1\n",
        "# Small constant to prevent any division by zero in the implementation (Adam)\n",
        "adam_epsilon = 1e-8\n",
        "# Number of steps for the warmup phase\n",
        "# Helps in avoiding very high and undesirable values of gradients at the start of training\n",
        "warmup_steps = 4\n",
        "# The split between the training and validation data\n",
        "training_validation_split = 0.85\n",
        "# Scales logits before soft-max to control randomness\n",
        "# Lower values (~0) make output more deterministic\n",
        "temperature = 0.5\n",
        "# Limits generated tokens to top K probabilities\n",
        "# Reduces chances of rare word predictions\n",
        "top_k = 50\n",
        "# Applies nucleus sampling, limiting token selection to a cumulative probability\n",
        "# Creates a balance between randomness and determinism\n",
        "top_p = 0.90"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Instantiate the T5 Variant Model\n",
        "\n",
        "Must be from Hugging Face or your local models directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from loguru import logger\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "logger.info(f\"Instantiating tokenizer from {tokenizer_path}, and model from {model_path}\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(f\"{tokenizer_path}\", model_max_length=max_input_token_length, add_special_tokens = False)\n",
        "input_model = T5ForConditionalGeneration.from_pretrained(f\"{model_path}\")\n",
        "logger.info(f\"Loading {model_path}...\")\n",
        "# Set device to be used based on GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Model is sent to device for use\n",
        "model = input_model.to(device) # type: ignore\n",
        "logger.success(\"Instantiated target tokenizer and model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The block below allows the user optionally to pull in the pre-trained and/or raw model checkpoint into this repository's local _forge/models/_ directory. This step is optional, but it allows [Step 2](#step-2-fine-tune-t5-checkpoint-model)'s user input to be a model in your local directory, thus providing offline usage and fine tuning later on. E.g., if you download google/flan-t5-xl to the local directory with name 'my-test-model' first, you can input '../models/my-test-model' for executing fine tuning on. The script below works on any model and tokenizer, but the fine tuning script in [Step 2](#step-2-fine-tune-t5-checkpoint-model) depends on the usage of a T5x variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model and tokenizer to your specified directory\n",
        "logger.info(f\"Downloading tokenizer from {tokenizer_path}, and model from {model_path}\")\n",
        "tokenizer.save_pretrained(f\"../models/{tokenizer_path.replace('/', '_')}\")\n",
        "input_model.save_pretrained(f\"../models/{model_path.replace('/', '_')}\") # type: ignore\n",
        "logger.success(\"Successfully downloaded target tokenizer and model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Fine Tune T5 Checkpoint Model\n",
        "\n",
        "If the target model is already fine-tuned or ready for manual testing, skip to [Step 4](#step-4-fine-tuned-model-manual-testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [],
      "source": [
        "# Fine tuning scripts\n",
        "import signal\n",
        "import re\n",
        "import traceback\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp.grad_scaler import GradScaler\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from scripts.file_utils import load_jsonl_data\n",
        "from scripts.constants import *\n",
        "from scripts.rich_logger import training_table as table, live_refresher\n",
        "\n",
        "model_output_directory = \"../models/\" + input(\n",
        "    \"What name would you like to give the fine-tuned model?\"\n",
        ")\n",
        "\n",
        "prompt_prefix_option = input(\n",
        "    \"Type the number to choose a prompt prefix type: (1) Bullet Prompt Training or (2) Data Creation Training\"\n",
        ")\n",
        "prompt_prefix = (\n",
        "    bullet_data_creation_prefix if prompt_prefix_option == \"2\" else bullet_prompt_prefix\n",
        ")\n",
        "data_set = (\n",
        "    \"../data/training/data_creation_set.jsonl\"\n",
        "    if prompt_prefix_option == \"2\"\n",
        "    else \"../data/training/training_validation_set.jsonl\"\n",
        ")\n",
        "\n",
        "data = load_jsonl_data(\n",
        "    data_set,\n",
        "    prompt_prefix,\n",
        "    isDataFrame=True,\n",
        ")\n",
        "\n",
        "\n",
        "# Creating a custom dataset for reading the dataset and loading it into the dataloader\n",
        "# to pass it to the neural network for fine tuning the model\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # Cleaning data so as to ensure data is in string type\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "# Generates a penalty for not complying to bullet formatting\n",
        "def format_penalty(outputs, tokenizer, format_pattern):\n",
        "    total_penalty = 0.0\n",
        "    logits = outputs.logits\n",
        "    # Converting the logits to token ids\n",
        "    token_ids = torch.argmax(logits, dim=-1)\n",
        "    # Decoding the token ids to text\n",
        "    decoded_outputs = [\n",
        "        tokenizer.decode(token_ids[i], skip_special_tokens=True)\n",
        "        for i in range(token_ids.shape[0])\n",
        "    ]\n",
        "\n",
        "    for text in decoded_outputs:\n",
        "        match = re.fullmatch(format_pattern, text)\n",
        "        # If the output does not match the desired format exactly, add a penalty\n",
        "        if not match:\n",
        "            total_penalty += 1.0\n",
        "\n",
        "    return torch.tensor(total_penalty, device=logits.device)\n",
        "\n",
        "    # Function to be called for training with the parameters passed from main function\n",
        "\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer, scheduler):\n",
        "    # Create a GradScaler object for mixed precision training\n",
        "    # Optionally define the GradScaler based on CUDA availability\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    scaler = GradScaler(enabled=use_cuda) if use_cuda else None\n",
        "\n",
        "    # Training logger refresh flag\n",
        "    table.switch_epoch_refresh()\n",
        "    # Stepping through training batches\n",
        "    for step, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Add a penalty to the loss for outputs that don't match the format\n",
        "        format_loss = format_penalty(outputs, tokenizer, bullet_pattern)\n",
        "        total_loss = loss + format_loss\n",
        "\n",
        "        if table.get_epoch_refresh():\n",
        "            # Refresh table once per epoch\n",
        "            table.refresh_table(epoch, loss)\n",
        "\n",
        "        if scaler:  # If CUDA is available and scaler is defined\n",
        "            scaler.scale(total_loss).backward() # type: ignore\n",
        "\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # type: ignore\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                # Clear gradients after optimizer step\n",
        "                optimizer.zero_grad()\n",
        "        else:  # If no CUDA, follow standard backward pass\n",
        "            total_loss.backward()\n",
        "\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # type: ignore\n",
        "                optimizer.step()\n",
        "\n",
        "                # Clear gradients after optimizer step\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        # Adjust the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Function to evaluate model for predictions and compute ROUGE scores\n",
        "def validate(tokenizer, model, device, loader):\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    # Initialize the rouge scorer\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                max_length=max_input_token_length,\n",
        "                num_beams=number_of_beams,\n",
        "                repetition_penalty=repetition_penalty,\n",
        "                length_penalty=length_penalty,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            preds = [\n",
        "                tokenizer.decode(\n",
        "                    g, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for g in generated_ids\n",
        "            ]\n",
        "            targets = [\n",
        "                tokenizer.decode(\n",
        "                    t, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for t in y\n",
        "            ]\n",
        "\n",
        "            # Calculate rouge scores for each prediction and corresponding target\n",
        "            for pred, target in zip(preds, targets):\n",
        "                score = scorer.score(target, pred)\n",
        "                scores.append(score)\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(targets)\n",
        "\n",
        "    # Compute the average ROUGE scores for the entire validation set\n",
        "    avg_scores = {\n",
        "        \"rouge1\": np.mean([score[\"rouge1\"].fmeasure for score in scores]),\n",
        "        \"rouge2\": np.mean([score[\"rouge2\"].fmeasure for score in scores]),\n",
        "        \"rougeL\": np.mean([score[\"rougeL\"].fmeasure for score in scores]),\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Average ROUGE scores: {avg_scores}\")\n",
        "\n",
        "    return predictions, actuals\n",
        "\n",
        "\n",
        "# T5 training main function\n",
        "def T5Trainer(dataframe, source_text, target_text):\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True # type: ignore\n",
        "\n",
        "    logger.info(\"Reading data...\")\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # 80% of the data will be used for training and the rest for validation\n",
        "    train_size = training_validation_split\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=seed)\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    logger.info(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    logger.info(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    logger.info(f\"VALIDATION Dataset: {val_dataset.shape}\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of data loader\n",
        "    training_set = CustomDataset(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        max_input_token_length,\n",
        "        max_output_token_length,\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = CustomDataset(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        max_input_token_length,\n",
        "        max_output_token_length,\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of data loaders\n",
        "    train_params = {\n",
        "        \"batch_size\": train_batch_size,\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "    val_params = {\n",
        "        \"batch_size\": valid_batch_size,\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of data loaders for testing and validation - this will be used down for training and validation stage for the model\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        params=[p for p in model.parameters() if p.requires_grad],\n",
        "        lr=learning_rate,\n",
        "        eps=adam_epsilon,\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "\n",
        "    # Define the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=train_epochs\n",
        "        * len(training_loader)\n",
        "        // gradient_accumulation_steps,\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    logger.info(f\"Initiating fine tuning of {model_path}...\")\n",
        "    # Table logger for training statistics\n",
        "    with live_refresher:\n",
        "        for epoch in range(train_epochs):\n",
        "            train(\n",
        "                epoch, tokenizer, model, device, training_loader, optimizer, scheduler\n",
        "            )\n",
        "    logger.info(f\"Saving fine-tuned  to {model_output_directory} ...\")\n",
        "    # Saving the model after training\n",
        "    save_model()\n",
        "\n",
        "    # Evaluating validation dataset\n",
        "    logger.info(\"Initiating validation...\")\n",
        "    for _ in range(val_epochs):\n",
        "        validate(tokenizer, model, device, val_loader)\n",
        "    logger.success(\"Model fine tuning, saving, and validation steps completed!\")\n",
        "\n",
        "\n",
        "# Saves the model\n",
        "def save_model():\n",
        "    model.save_pretrained(model_output_directory)\n",
        "    tokenizer.save_pretrained(model_output_directory)\n",
        "    logger.info(f\"Fine-tuned model successfully saved to: {model_output_directory}\")\n",
        "    logger.success(\"Model saved. Shutting down...\")\n",
        "\n",
        "\n",
        "# In case of interrupt, save model and exit\n",
        "def save_and_exit(signal, _):\n",
        "    logger.warning(\n",
        "        f\"Received interrupt signal {signal}, stopping script and saving model...\"\n",
        "    )\n",
        "    save_model()\n",
        "\n",
        "\n",
        "# Attach the SIGINT signal (generated by Ctrl+C) to the handler\n",
        "signal.signal(signal.SIGINT, save_and_exit)\n",
        "\n",
        "try:\n",
        "    # Run training function on the T5 model using data set and training parameters\n",
        "    T5Trainer(dataframe=data, source_text=\"input\", target_text=\"output\")\n",
        "except Exception as e:\n",
        "    # Handle other unexpected errors\n",
        "    logger.error(f\"An unexpected error occurred during fine tuning: {str(e)}\")\n",
        "    logger.error(f\"Stack trace: \\n{traceback.format_exc()}\")\n",
        "    # Save the model and any relevant data before exiting gracefully\n",
        "    save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Fine Tuned Model Manual Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scripts.constants import bullet_prompt_prefix, bullet_data_creation_prefix\n",
        "\n",
        "# Load the data from the manual test file\n",
        "prompt_prefix_option = input(\n",
        "    \"Type the number to choose a prompt prefix type: (1) Bullet Prompt or (2) Data Creation\"\n",
        ")\n",
        "prompt_prefix = (\n",
        "    bullet_data_creation_prefix if prompt_prefix_option == \"2\" else bullet_prompt_prefix\n",
        ")\n",
        "\n",
        "# Preprocess input\n",
        "input_text = prompt_prefix + input(\"Provide your input below, sans prompt prefix.\")\n",
        "\n",
        "encoded_input_text = tokenizer.encode_plus(\n",
        "    input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_token_length\n",
        ")\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = model.generate(\n",
        "    encoded_input_text[\"input_ids\"],\n",
        "    attention_mask=encoded_input_text[\"attention_mask\"],\n",
        "    max_length=max_output_token_length,\n",
        "    num_beams=number_of_beams,\n",
        "    temperature=temperature,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "output_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print results\n",
        "logger.info(f\"INPUT: {input_text}\")\n",
        "logger.success(f\"GENERATED OUTPUT: {output_text}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+sqU2Hgca8RM/Wjv+9kvQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "T5 Fine tuning with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
