{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Forge Model Fine Tuning: T5x Variants\n",
        "\n",
        "This notebook is specifically geared towards the fine tuning of T5x variant models using PyTorch and Hugging Face Transformers. the notebook contains model fine tuning and monitoring scripts to be used for the generation of the Forge's training data set and the production Forge model.\n",
        "\n",
        "This notebook includes options, through user input, to choose specific checkpoint models and tokenizers and choices for directing the fine tuning or inferencing towards bullet interpretation or bullet generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Dependencies\n",
        "\n",
        "This block must be run prior to performing any of the steps following this one.\n",
        "\n",
        "This block imports all the necessary scripts and dependencies used by all of the following blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import signal\n",
        "import traceback\n",
        "import torch\n",
        "from loguru import logger\n",
        "\n",
        "from scripts.constants import *\n",
        "from scripts.model_instantiation import *\n",
        "from scripts.file_utils import load_jsonl_as_dataframe\n",
        "from scripts.prompt import prompt, select_prompt_type\n",
        "from scripts.t5_trainer import T5Trainer\n",
        "from scripts.training import save_trained_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Instantiate the Model and Tokenizer\n",
        "\n",
        "This block instantiates a selected model and tokenizer, and loads it to the target device for further usage (e.g., fine tuning, inferencing, etc.).\n",
        "\n",
        "The block below also allows the user optionally to pull in the pre-trained and/or raw model checkpoint into this repository's local _forge/models/_ directory, providing offline usage and fine tuning later on.\n",
        "\n",
        "When selecting a model and tokenizer, ensure the tokenizer is of the same signature and base model as the selected model. For example, if using google/flan-t5-xl as the model then the tokenizer should be google/flan-t5-base.\n",
        "\n",
        "Some T5x variant checkpoints to try out for fine tuning or inferencing include, but are note limited to:\n",
        "- [MBZUAI/LaMini-T5-738M](https://huggingface.co/MBZUAI/LaMini-T5-738M)\n",
        "- [t5-base](https://huggingface.co/t5-base)\n",
        "- [google/t5-efficient-tiny](https://huggingface.co/google/t5-efficient-tiny)\n",
        "\n",
        " The block requires the following user input:\n",
        "\n",
        "- model_path: The model's directory or Hugging Face repository, e.g., google/flan-t5-xl, ../models/opera-bullet-interpreter\n",
        "- tokenizer_path: The tokenizer's directory or Hugging Face repository, e.g., google/flan-t5-xl, ../models/opera-bullet-interpreter\n",
        "- save_model_decision: Whether the user wants to save a copy of the model to the local directory, \"yes\" or \"no\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_path, model_path = select_model()\n",
        "tokenizer, model = load_model(model_path, tokenizer_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Fine Tune T5 Checkpoint Model\n",
        "\n",
        "If the target model is already fine-tuned or ready for manual testing, skip to [Step 4](#step-4-fine-tuned-model-manual-testing)\n",
        "\n",
        " The block requires the following input:\n",
        "\n",
        "- model_output_directory: The fine-tuned model's destination directory, e.g., ../models/opera-bullet-interpreter\n",
        "- prompt_prefix_option: The number corresponding to the type of model you want to train, either a model for creating long-form sentences from an existing bullet or a model that creates new bullets from long-form sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [],
      "source": [
        "# Set the device for loading the model to\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_output_directory = \"../models/\" + input(\n",
        "    \"What name would you like to give the fine-tuned model?\"\n",
        ")\n",
        "\n",
        "data_set = \"../data/training/bullet_training_set.jsonl\"\n",
        "\n",
        "prompt_prefix, prompt_prefix_option = select_prompt_type()\n",
        "\n",
        "data = load_jsonl_as_dataframe(data_set, prompt_prefix)\n",
        "\n",
        "\n",
        "# In case of interrupt, save model and exit\n",
        "def save_and_exit(signal, _):\n",
        "    logger.warning(\n",
        "        f\"Received interrupt (code {signal}), stopping script and saving progress...\"\n",
        "    )\n",
        "    save_trained_model()\n",
        "\n",
        "\n",
        "# Attach the SIGINT signal (generated by Ctrl+C) to the handler\n",
        "signal.signal(signal.SIGINT, save_and_exit)\n",
        "\n",
        "# Run training function on the T5 model using data set and training parameters\n",
        "# Prompt prefix of (1) is Bullet Creation Training and (2) is Bullet Interpretation Training\n",
        "# Input key stores the bullet, Output key stores the long-form sentence(s)\n",
        "if prompt_prefix_option == \"2\":\n",
        "    # Train the bullet interpreter model\n",
        "    T5Trainer(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        model_output_directory,\n",
        "        dataframe=data,\n",
        "        source_text=\"input\",\n",
        "        target_text=\"output\",\n",
        "    )\n",
        "else:\n",
        "    # Train the bullet creator model\n",
        "    T5Trainer(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        model_output_directory,\n",
        "        dataframe=data,\n",
        "        source_text=\"output\",\n",
        "        target_text=\"input\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Fine Tuned Model Manual Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The block requires the following input:\n",
        "\n",
        "- prompt_prefix_option: The number corresponding to the type of model you want to inference, either a model for creating long-form sentences from an existing bullet or a model that creates new bullets from long-form sentences\n",
        "- input_text: The input you are providing to the model to respond to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_prefix_option = input(\n",
        "    \"Type the number to choose a prompt prefix type: (1) Bullet Prompt or (2) Data Creation\"\n",
        ")\n",
        "prompt_prefix = (\n",
        "    bullet_data_creation_prefix if prompt_prefix_option == \"2\" else bullet_prompt_prefix\n",
        ")\n",
        "\n",
        "# Preprocess input\n",
        "input_text = prompt_prefix + input(\"Provide your input below, sans prompt prefix.\")\n",
        "\n",
        "output_text = prompt(model, tokenizer, input_text)\n",
        "\n",
        "# Print results\n",
        "logger.info(f\"INPUT: {input_text}\")\n",
        "logger.success(f\"GENERATED OUTPUT: {output_text}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+sqU2Hgca8RM/Wjv+9kvQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "T5 Fine tuning with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
