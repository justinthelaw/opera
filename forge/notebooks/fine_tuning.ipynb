{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bullet Forge Model Fine Tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Instantiate Global Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What is the max length of tokens a user may enter for summarization\n",
        "max_input_token_length = 512\n",
        "# What is the max length of tokens the model should output for the summary\n",
        "max_output_token_length = 35"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fine Tune Checkpoint Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-07-15 15:10:31.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mT5Trainer\u001b[0m:\u001b[36m214\u001b[0m - \u001b[1mLoading t5-small...\u001b[0m\n",
            "\u001b[32m2023-07-15 15:10:34.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mT5Trainer\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mReading data...\u001b[0m\n",
            "\u001b[32m2023-07-15 15:10:34.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mT5Trainer\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mFULL Dataset: (400, 2)\u001b[0m\n",
            "\u001b[32m2023-07-15 15:10:34.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mT5Trainer\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mTRAIN Dataset: (320, 2)\u001b[0m\n",
            "\u001b[32m2023-07-15 15:10:34.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mT5Trainer\u001b[0m:\u001b[36m239\u001b[0m - \u001b[1mVALIDATION Dataset: (80, 2)\u001b[0m\n",
            "\u001b[32m2023-07-15 15:10:34.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mT5Trainer\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mInitiating fine tuning...\u001b[0m\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 338\u001b[0m\n\u001b[1;32m    316\u001b[0m model_params \u001b[39m=\u001b[39m {\n\u001b[1;32m    317\u001b[0m     \u001b[39m# model_type: t5-x\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMODEL\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00minput_model\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSEED\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m42\u001b[39m,\n\u001b[1;32m    335\u001b[0m }\n\u001b[1;32m    337\u001b[0m \u001b[39m# Run training function on the T5 model using data set and training parameters\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m T5Trainer(\n\u001b[1;32m    339\u001b[0m     dataframe\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    340\u001b[0m     source_text\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    341\u001b[0m     target_text\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    342\u001b[0m     model_params\u001b[39m=\u001b[39;49mmodel_params,\n\u001b[1;32m    343\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_output_directory\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    344\u001b[0m )\n",
            "Cell \u001b[0;32mIn[6], line 291\u001b[0m, in \u001b[0;36mT5Trainer\u001b[0;34m(dataframe, source_text, target_text, model_params, output_dir)\u001b[0m\n\u001b[1;32m    288\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInitiating fine tuning...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(model_params[\u001b[39m\"\u001b[39m\u001b[39mTRAIN_EPOCHS\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m--> 291\u001b[0m     train(epoch, tokenizer, model, device, training_loader, optimizer, scheduler)\n\u001b[1;32m    292\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    294\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSaving model...\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 130\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, tokenizer, model, device, loader, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    127\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[39m# Add a penalty to the loss for outputs that don't match the format\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m format_loss \u001b[39m=\u001b[39m format_penalty(outputs, tokenizer, BULLET_PATTERN)\n\u001b[1;32m    131\u001b[0m total_loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m format_loss\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m _ \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "Cell \u001b[0;32mIn[6], line 96\u001b[0m, in \u001b[0;36mformat_penalty\u001b[0;34m(outputs, tokenizer, format_pattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     95\u001b[0m \u001b[39m# Decoding the logits to text\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m decoded_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     97\u001b[0m     tokenizer\u001b[39m.\u001b[39;49mdecode(logits[i], skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     98\u001b[0m     \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(logits\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     99\u001b[0m ]\n\u001b[1;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m decoded_outputs:\n\u001b[1;32m    102\u001b[0m     match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfullmatch(format_pattern, text)\n",
            "Cell \u001b[0;32mIn[6], line 97\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     95\u001b[0m \u001b[39m# Decoding the logits to text\u001b[39;00m\n\u001b[1;32m     96\u001b[0m decoded_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 97\u001b[0m     tokenizer\u001b[39m.\u001b[39;49mdecode(logits[i], skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(logits\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m     99\u001b[0m ]\n\u001b[1;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m decoded_outputs:\n\u001b[1;32m    102\u001b[0m     match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfullmatch(format_pattern, text)\n",
            "File \u001b[0;32m~/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3509\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3506\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3507\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3509\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3510\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3511\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3512\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3513\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3514\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py:931\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[1;32m    922\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    923\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    928\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    933\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n",
            "File \u001b[0;32m~/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py:906\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[0;32m--> 906\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[1;32m    908\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from loguru import logger\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import cuda\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "from scripts.files import load_jsonl_data\n",
        "from scripts.bullet_patterns import *\n",
        "from scripts.rich_logger import *\n",
        "\n",
        "input_model = input(\n",
        "    \"What is the target model's directory path or checkpoint name on Hugging Face?\"\n",
        ")\n",
        "model_output_directory = \"../models/\" + input(\n",
        "    \"What name would you like to give the fine-tuned model?\"\n",
        ")\n",
        "\n",
        "# Load JSONLdata\n",
        "data = load_jsonl_data(\n",
        "    \"../data/training/training_validation_set.jsonl\",\n",
        "    BULLET_PROMPT_PREFIX,\n",
        "    isDataFrame=True,\n",
        ")\n",
        "\n",
        "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# Creating a custom dataset for reading the dataset and loading it into the dataloader\n",
        "# to pass it to the neural network for fine tuning the model\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # Cleaning data so as to ensure data is in string type\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "# Generates a penalty for not complying to bullet formatting\n",
        "def format_penalty(outputs, tokenizer, format_pattern):\n",
        "    total_penalty = 0.0\n",
        "    logits = outputs.logits\n",
        "    # Converting the logits to token ids\n",
        "    token_ids = torch.argmax(logits, dim=-1)\n",
        "    # Decoding the token ids to text\n",
        "    decoded_outputs = [tokenizer.decode(token_ids[i], skip_special_tokens=True) for i in range(token_ids.shape[0])]\n",
        "    \n",
        "    for text in decoded_outputs:\n",
        "        match = re.fullmatch(format_pattern, text)\n",
        "        # If the output does not match the desired format exactly, add a penalty\n",
        "        if not match:\n",
        "            total_penalty += 1.0\n",
        "\n",
        "    return torch.tensor(total_penalty, device=logits.device)\n",
        "\n",
        "\n",
        "\n",
        "# Function to be called for training with the parameters passed from main function\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer, scheduler):\n",
        "    model.train()\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Add a penalty to the loss for outputs that don't match the format\n",
        "        format_loss = format_penalty(outputs, tokenizer, BULLET_PATTERN)\n",
        "        total_loss = loss + format_loss\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            training_logger.add_row(str(epoch + 1), str(_), str(total_loss))\n",
        "            general_logger.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Function to evaluate model for predictions and compute ROUGE scores\n",
        "def validate(tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    # Initialize the rouge scorer\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                max_length=max_input_token_length,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            preds = [\n",
        "                tokenizer.decode(\n",
        "                    g, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for g in generated_ids\n",
        "            ]\n",
        "            targets = [\n",
        "                tokenizer.decode(\n",
        "                    t, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for t in y\n",
        "            ]\n",
        "\n",
        "            # Calculate rouge scores for each prediction and corresponding target\n",
        "            for pred, target in zip(preds, targets):\n",
        "                score = scorer.score(target, pred)\n",
        "                scores.append(score)\n",
        "\n",
        "            if _ % 10 == 0:\n",
        "                general_logger.print(f\"Completed {_}\")\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(targets)\n",
        "\n",
        "    # Compute the average ROUGE scores for the entire validation set\n",
        "    avg_scores = {\n",
        "        \"rouge1\": np.mean([score[\"rouge1\"].fmeasure for score in scores]),\n",
        "        \"rouge2\": np.mean([score[\"rouge2\"].fmeasure for score in scores]),\n",
        "        \"rougeL\": np.mean([score[\"rougeL\"].fmeasure for score in scores]),\n",
        "    }\n",
        "\n",
        "    logger.print(f\"Average ROUGE scores: {avg_scores}\")\n",
        "\n",
        "    return predictions, actuals\n",
        "\n",
        "\n",
        "# T5 training main function\n",
        "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir):\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])\n",
        "    np.random.seed(model_params[\"SEED\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    logger.info(f\"Loading {model_params['MODEL']}...\")\n",
        "\n",
        "    # Tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\n",
        "        model_params[\"MODEL\"], model_max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"]\n",
        "    )\n",
        "\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of summary\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "    logger.info(f\"Reading data...\")\n",
        "\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # 80% of the data will be used for training and the rest for validation\n",
        "    train_size = 0.8\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    logger.info(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    logger.info(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    logger.info(f\"VALIDATION Dataset: {val_dataset.shape}\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of data loader\n",
        "    training_set = CustomDataset(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = CustomDataset(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of data loaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of data loaders for testing and validation - this will be used down for training and validation stage for the model\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    # Define the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=len(training_loader) * model_params[\"TRAIN_EPOCHS\"],\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    logger.info(f\"Initiating fine tuning...\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer, scheduler)\n",
        "        scheduler.step()\n",
        "\n",
        "    logger.info(f\"Saving model...\")\n",
        "    # Saving the model after training\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # Evaluating validation dataset\n",
        "    logger.info(f\"Initiating validation...\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "\n",
        "    general_logger.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
        "\n",
        "    logger.success(f\"Model validation completed!\")\n",
        "    logger.info(f\"Fine-tuned model saved to: {output_dir}\")\n",
        "    logger.info(\n",
        "        f\"Validation data saved to: {os.path.join(output_dir,'predictions.csv')}\"\n",
        "    )\n",
        "    logger.info(f\"Notebook logs saved to: {os.path.join(output_dir,'logs.txt')}\")\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    # model_type: t5-x\n",
        "    \"MODEL\": f\"{input_model}\",\n",
        "    # training batch size\n",
        "    \"TRAIN_BATCH_SIZE\": 16,\n",
        "    # validation batch size\n",
        "    \"VALID_BATCH_SIZE\": 16,\n",
        "    # number of training epochs\n",
        "    \"TRAIN_EPOCHS\": 8,\n",
        "    # number of validation epochs\n",
        "    \"VAL_EPOCHS\": 1,\n",
        "    # learning rate\n",
        "    \"LEARNING_RATE\": 1e-4,\n",
        "    # max length of source text\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": max_input_token_length,\n",
        "    # max length of target text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": max_output_token_length,\n",
        "    # set seed for reproducibility\n",
        "    \"SEED\": 42,\n",
        "}\n",
        "\n",
        "# Run training function on the T5 model using data set and training parameters\n",
        "T5Trainer(\n",
        "    dataframe=data,\n",
        "    source_text=\"input\",\n",
        "    target_text=\"output\",\n",
        "    model_params=model_params,\n",
        "    output_dir=f\"{model_output_directory}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Fine Tuned Model Manual Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "from scripts.files import load_jsonl_data\n",
        "from scripts.bullet_patterns import BULLET_PROMPT_PREFIX\n",
        "\n",
        "input_model = input(\"What is the target model's directory path or checkpoint name on Hugging Face?\")\n",
        "\n",
        "# Load the T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained(f\"{input_model}\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", model_max_length=max_output_token_length)\n",
        "\n",
        "# Load the data from the manual test file\n",
        "data = load_jsonl_data(\"../data/training/manual_test_set.jsonl\", BULLET_PROMPT_PREFIX, isDataFrame=False)\n",
        "for line in data:\n",
        "    # Preprocess input\n",
        "    input_text = line[\"input\"]\n",
        "    expected_summary = line[\"output\"]\n",
        "\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        input_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
        "    )\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        # Adjust the max length according to your desired summary length\n",
        "        max_length=max_output_token_length,\n",
        "        # Adjust the number of beams for beam search\n",
        "        num_beams=2,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"> INPUT TEXT: {input_text}\")\n",
        "    print(f\"\\t> EXPECTED SUMMARY: {expected_summary}\")\n",
        "    print(f\"\\t> GENERATE SUMMARY: {summary}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+sqU2Hgca8RM/Wjv+9kvQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "T5 Fine tuning with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
