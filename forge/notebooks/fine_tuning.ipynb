{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y6nEben93JAk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>I rebuilt my unit's Electronic Records Managem...</td>\n",
              "      <td>- Rebuilt unit's Electronic Records Management...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>This individual led the development of six upd...</td>\n",
              "      <td>- Led development for six PEPP/AIMWTS updates;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>I developed 7 analysis tools that raised comma...</td>\n",
              "      <td>- Dev'd 7 analysis tools; raised cmd awareness...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>As a well-rounded Noncommissioned Officer (NCO...</td>\n",
              "      <td>- Well-rounded NCO; will produce exceptional r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>Serving as a member of the alert photo team, I...</td>\n",
              "      <td>- Member of alert photo team; responded to 5 c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>By completing a 40-hour Faculty Development co...</td>\n",
              "      <td>- Comp 40 hr Fac Dev crse; enhanced tech mat/w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>I revamped the HVAC (Heating, Ventilation, and...</td>\n",
              "      <td>- Revamped HVAC system; built and installed 6,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>I excelled in the EPME/ALS 6-week, 240-hour co...</td>\n",
              "      <td>- Excelled EPME/ALS 6 wk/240 hrs crse; rcv'd c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>I led the Small Package Initial Communications...</td>\n",
              "      <td>- Led Small Pkg Initial Comm Element 'Rodeo' t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>I successfully drained and purged 7 excess veh...</td>\n",
              "      <td>- Drained/purged 7 excess vehicles/$1.3M; DLA-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 input  \\\n",
              "229  I rebuilt my unit's Electronic Records Managem...   \n",
              "309  This individual led the development of six upd...   \n",
              "59   I developed 7 analysis tools that raised comma...   \n",
              "222  As a well-rounded Noncommissioned Officer (NCO...   \n",
              "118  Serving as a member of the alert photo team, I...   \n",
              "111  By completing a 40-hour Faculty Development co...   \n",
              "284  I revamped the HVAC (Heating, Ventilation, and...   \n",
              "56   I excelled in the EPME/ALS 6-week, 240-hour co...   \n",
              "51   I led the Small Package Initial Communications...   \n",
              "10   I successfully drained and purged 7 excess veh...   \n",
              "\n",
              "                                                output  \n",
              "229  - Rebuilt unit's Electronic Records Management...  \n",
              "309  - Led development for six PEPP/AIMWTS updates;...  \n",
              "59   - Dev'd 7 analysis tools; raised cmd awareness...  \n",
              "222  - Well-rounded NCO; will produce exceptional r...  \n",
              "118  - Member of alert photo team; responded to 5 c...  \n",
              "111  - Comp 40 hr Fac Dev crse; enhanced tech mat/w...  \n",
              "284  - Revamped HVAC system; built and installed 6,...  \n",
              "56   - Excelled EPME/ALS 6 wk/240 hrs crse; rcv'd c...  \n",
              "51   - Led Small Pkg Initial Comm Element 'Rodeo' t...  \n",
              "10   - Drained/purged 7 excess vehicles/$1.3M; DLA-...  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas\n",
        "\n",
        "# Read in the JSONL training and validation data set\n",
        "data = pandas.read_json(\"../data/training/training_validation_set.jsonl\", lines=True)\n",
        "# Show random sample of 10 data sets\n",
        "data.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AYfBicZQ59Jf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>summarize: I effectively managed the vehicle r...</td>\n",
              "      <td>- Managed 57 volunteers vehicle requirements; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>summarize: I was handpicked for the Fighter Wi...</td>\n",
              "      <td>- Handpicked for FW Civic Leader event; prepar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>summarize: I repaired damaged Weapons System S...</td>\n",
              "      <td>- Repaired damaged WSS training munitions; rep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>summarize: By writing, testing, and applying a...</td>\n",
              "      <td>- Wrote/tested/applied new PM print program; a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>summarize: As a Total Force champion, I certif...</td>\n",
              "      <td>- Total Force champion; certified 20 AFRC/ANG ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>summarize: Taking the lead as the quarterback ...</td>\n",
              "      <td>- Quarterbacked cooling duct TCTO; designed dr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>summarize: As a custodian account manager, I e...</td>\n",
              "      <td>- Managed 22 custody accounts; outlined provis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>summarize: I coordinated fire support for 180 ...</td>\n",
              "      <td>- Coordinated fire support to 180 test mission...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>summarize: With a strong focus on safety, I vo...</td>\n",
              "      <td>- Safety conscious; volunteered as 'Top III' d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>summarize: I completed training in pylon remov...</td>\n",
              "      <td>- Completed training in pylon removal for TCTO...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 input  \\\n",
              "26   summarize: I effectively managed the vehicle r...   \n",
              "216  summarize: I was handpicked for the Fighter Wi...   \n",
              "205  summarize: I repaired damaged Weapons System S...   \n",
              "315  summarize: By writing, testing, and applying a...   \n",
              "253  summarize: As a Total Force champion, I certif...   \n",
              "155  summarize: Taking the lead as the quarterback ...   \n",
              "187  summarize: As a custodian account manager, I e...   \n",
              "321  summarize: I coordinated fire support for 180 ...   \n",
              "176  summarize: With a strong focus on safety, I vo...   \n",
              "186  summarize: I completed training in pylon remov...   \n",
              "\n",
              "                                                output  \n",
              "26   - Managed 57 volunteers vehicle requirements; ...  \n",
              "216  - Handpicked for FW Civic Leader event; prepar...  \n",
              "205  - Repaired damaged WSS training munitions; rep...  \n",
              "315  - Wrote/tested/applied new PM print program; a...  \n",
              "253  - Total Force champion; certified 20 AFRC/ANG ...  \n",
              "155  - Quarterbacked cooling duct TCTO; designed dr...  \n",
              "187  - Managed 22 custody accounts; outlined provis...  \n",
              "321  - Coordinated fire support to 180 test mission...  \n",
              "176  - Safety conscious; volunteered as 'Top III' d...  \n",
              "186  - Completed training in pylon removal for TCTO...  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepend T5's summarize task keyword to inputs\n",
        "data[\"input\"] = \"summarize: \"+data[\"input\"]\n",
        "# Show random sample of 10 data sets\n",
        "data.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# Define a rich console logger\n",
        "console = Console(record=True)\n",
        "\n",
        "# Setup the data frame display\n",
        "def display_df(df):\n",
        "    console = Console()\n",
        "    table = Table(\n",
        "        Column(\"source_text\", justify=\"center\"),\n",
        "        Column(\"target_text\", justify=\"center\"),\n",
        "        title=\"Sample Data\",\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "    )\n",
        "\n",
        "    for _, row in enumerate(df.values.tolist()):\n",
        "        table.add_row(row[0], row[1])\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "outputs": [],
      "source": [
        "# Setting up the device for GPU usage, if available\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Creating a custom dataset for reading the dataset and loading it into the dataloader \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# to pass it to the neural network for fine tuning the model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mYourDataSetClass\u001b[39;00m(Dataset):\n\u001b[1;32m      5\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n\u001b[1;32m      6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Creating a custom dataset for reading the dataset and loading it into the dataloader \n",
        "# to pass it to the neural network for fine tuning the model\n",
        "class YourDataSetClass(Dataset):\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.summ_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "\n",
        "    #cleaning data so as to ensure data is in string type\n",
        "    source_text = ' '.join(source_text.split())\n",
        "    target_text = ' '.join(target_text.split())\n",
        "\n",
        "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids': source_ids.to(dtype=torch.long), \n",
        "        'source_mask': source_mask.to(dtype=torch.long), \n",
        "        'target_ids': target_ids.to(dtype=torch.long),\n",
        "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Nkj6wIMt40RK"
      },
      "outputs": [],
      "source": [
        "# Function to be called for training with the parameters passed from main function\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "            console.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GUBykK-A43DF"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model for predictions\n",
        "def validate(tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                max_length=150,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            predictions = [\n",
        "                tokenizer.decode(\n",
        "                    g, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for g in generated_ids\n",
        "            ]\n",
        "            target = [\n",
        "                tokenizer.decode(\n",
        "                    t, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for t in y\n",
        "            ]\n",
        "            if _ % 10 == 0:\n",
        "                console.print(f\"Completed {_}\")\n",
        "\n",
        "            predictions.extend(predictions)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tw4RW_qO4_8T"
      },
      "outputs": [],
      "source": [
        "# T5 training main function\n",
        "def T5Trainer(\n",
        "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"\n",
        "):\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])\n",
        "    np.random.seed(model_params[\"SEED\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    # Tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"], model_max_length=512)\n",
        "\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of summary\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "    console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "    display_df(dataframe.head(2))\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # 80% of the data will be used for training and the rest for validation\n",
        "    train_size = 0.8\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    console.print(f\"VALIDATION Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of data loader\n",
        "    training_set = YourDataSetClass(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = YourDataSetClass(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of data loaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of data loaders for testing and validation - this will be used down for training and validation stage for the model\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    console.log(f\"[Saving Model]...\\n\")\n",
        "    # Saving the model after training\n",
        "    path = os.path.join(output_dir, \"model_files\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "    # Evaluating validation dataset\n",
        "    console.log(f\"[Initiating Validation]...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "\n",
        "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
        "\n",
        "    console.log(f\"[Validation Completed.]\\n\")\n",
        "    console.print(\n",
        "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
        "    )\n",
        "    console.print(\n",
        "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
        "    )\n",
        "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PxCpQwD8PDIs"
      },
      "outputs": [],
      "source": [
        "model_params = {\n",
        "    # model_type: t5-base/t5-large\n",
        "    \"MODEL\": \"t5-large\",\n",
        "    # training batch size\n",
        "    \"TRAIN_BATCH_SIZE\": 8,\n",
        "    # validation batch size\n",
        "    \"VALID_BATCH_SIZE\": 8,\n",
        "    # number of training epochs\n",
        "    \"TRAIN_EPOCHS\": 3,\n",
        "    # number of validation epochs\n",
        "    \"VAL_EPOCHS\": 1,\n",
        "    # learning rate\n",
        "    \"LEARNING_RATE\": 1e-4,\n",
        "    # max length of source text\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 512,\n",
        "    # max length of target text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 50,\n",
        "    # set seed for reproducibility\n",
        "    \"SEED\": 42,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qijZoYeI55fM",
        "outputId": "69c68bb6-4fba-47e4-9e74-73f2579aa3c8"
      },
      "outputs": [],
      "source": [
        "output_directory = input(\"Insert the directory for the model\")\n",
        "\n",
        "# Run training function on the T5 model using data set and training parameters\n",
        "T5Trainer(dataframe=data[:500], source_text=\"input\", target_text=\"output\", model_params=model_params, output_dir=f\"{output_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Expected Summary:  - Co-authored 64 pg SABER Guide; used to train CONS personnel--120 mnhrs saved/identified as ORI Wing strength\n",
            "> Generated Summary:  co-author of 64-page SABER Guide used for training CONS personnel. guide saved 120 manpower hours and recognized as strength during ORI\n"
          ]
        }
      ],
      "source": [
        "# Load the T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained('../models/t5-large')\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)\n",
        "\n",
        "# Preprocess input\n",
        "input_text = input(\"Insert a statement to summarize\")\n",
        "input_expected_summary = input(\"Insert the expected statement summary\")\n",
        "prefix = \"summarize: \"\n",
        "input_text = prefix + input_text\n",
        "\n",
        "inputs = tokenizer.encode_plus(input_text, return_tensors='pt', truncation=True, max_length=512)\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    attention_mask=inputs['attention_mask'],\n",
        "    # Adjust the max length according to your desired summary length\n",
        "    max_length=150,\n",
        "    # Adjust the number of beams for beam search\n",
        "    num_beams=20,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"> Expected Summary: \", input_expected_summary)\n",
        "print(\"> Generated Summary: \", summary)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+sqU2Hgca8RM/Wjv+9kvQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "T5 Fine tuning with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
