{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bullet Forge Model Fine Tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Instantiate Global Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_INPUT_TOKEN_LENGTH = 512\n",
        "MAX_OUTPUT_TOKEN_LENGTH = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fine Tune Checkpoint Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "from scripts.utils.files import load_jsonl_data\n",
        "from classes.DataSet import DataSet\n",
        "\n",
        "model_checkpoint = input(\"Input the t5 model checkpoint name to be fine tuned\")\n",
        "directory_suffix = input(\"Input extra suffix to name of model output\")\n",
        "\n",
        "# Load JSONLdata\n",
        "data = load_jsonl_data(\n",
        "    \"../data/training/training_validation_set.jsonl\", isDataFrame=True\n",
        ")\n",
        "\n",
        "# Define a rich console logger\n",
        "console = Console(record=True)\n",
        "\n",
        "\n",
        "# Setup the data frame display\n",
        "def display_df(df):\n",
        "    console = Console()\n",
        "    table = Table(\n",
        "        Column(\"source_text\", justify=\"center\"),\n",
        "        Column(\"target_text\", justify=\"center\"),\n",
        "        title=\"Sample Data\",\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "    )\n",
        "\n",
        "    for _, row in enumerate(df.values.tolist()):\n",
        "        table.add_row(row[0], row[1])\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")\n",
        "\n",
        "# Setting up the device for GPU usage, if available\n",
        "from torch import cuda\n",
        "\n",
        "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# Function to be called for training with the parameters passed from main function\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            training_logger.add_row(str(epoch + 1), str(_), str(loss))\n",
        "            console.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# Function to evaluate model for predictions and compute ROUGE scores\n",
        "def validate(tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    # Initialize the rouge scorer\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                max_length=MAX_INPUT_TOKEN_LENGTH,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            preds = [\n",
        "                tokenizer.decode(\n",
        "                    g, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for g in generated_ids\n",
        "            ]\n",
        "            targets = [\n",
        "                tokenizer.decode(\n",
        "                    t, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for t in y\n",
        "            ]\n",
        "\n",
        "            # Calculate rouge scores for each prediction and corresponding target\n",
        "            for pred, target in zip(preds, targets):\n",
        "                score = scorer.score(target, pred)\n",
        "                scores.append(score)\n",
        "\n",
        "            if _ % 10 == 0:\n",
        "                console.print(f\"Completed {_}\")\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(targets)\n",
        "\n",
        "    # Compute the average ROUGE scores for the entire validation set\n",
        "    avg_scores = {\n",
        "        \"rouge1\": np.mean([score[\"rouge1\"].fmeasure for score in scores]),\n",
        "        \"rouge2\": np.mean([score[\"rouge2\"].fmeasure for score in scores]),\n",
        "        \"rougeL\": np.mean([score[\"rougeL\"].fmeasure for score in scores]),\n",
        "    }\n",
        "\n",
        "    console.print(f\"Average ROUGE scores: {avg_scores}\")\n",
        "\n",
        "    return predictions, actuals\n",
        "\n",
        "\n",
        "# T5 training main function\n",
        "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir):\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])\n",
        "    np.random.seed(model_params[\"SEED\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    # Tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"], model_max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"])\n",
        "\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of summary\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "    console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "    display_df(dataframe.head(2))\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # 80% of the data will be used for training and the rest for validation\n",
        "    train_size = 0.8\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    console.print(f\"VALIDATION Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of data loader\n",
        "    training_set = DataSet(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = DataSet(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of data loaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of data loaders for testing and validation - this will be used down for training and validation stage for the model\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    console.log(f\"[Saving Model]...\\n\")\n",
        "    # Saving the model after training\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # Evaluating validation dataset\n",
        "    console.log(f\"[Initiating Validation]...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "\n",
        "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
        "\n",
        "    console.log(f\"[Validation Completed.]\\n\")\n",
        "    console.print(f\"\"\"[Model] Model saved @ {output_dir}\\n\"\"\")\n",
        "    console.print(\n",
        "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
        "    )\n",
        "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    # model_type: t5-x\n",
        "    \"MODEL\": f\"{model_checkpoint}\",\n",
        "    # training batch size\n",
        "    \"TRAIN_BATCH_SIZE\": 4,\n",
        "    # validation batch size\n",
        "    \"VALID_BATCH_SIZE\": 4,\n",
        "    # number of training epochs\n",
        "    \"TRAIN_EPOCHS\": 4,\n",
        "    # number of validation epochs\n",
        "    \"VAL_EPOCHS\": 1,\n",
        "    # learning rate\n",
        "    \"LEARNING_RATE\": 1e-5,\n",
        "    # max length of source text\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": MAX_INPUT_TOKEN_LENGTH,\n",
        "    # max length of target text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": MAX_OUTPUT_TOKEN_LENGTH,\n",
        "    # set seed for reproducibility\n",
        "    \"SEED\": 42,\n",
        "}\n",
        "\n",
        "output_directory = f\"../models/pytorch-cpu-{model_checkpoint}{directory_suffix}\"\n",
        "\n",
        "# Run training function on the T5 model using data set and training parameters\n",
        "T5Trainer(\n",
        "    dataframe=data,\n",
        "    source_text=\"input\",\n",
        "    target_text=\"output\",\n",
        "    model_params=model_params,\n",
        "    output_dir=f\"{output_directory}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Fine Tuned Model Manual Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from scripts.utils.files import load_jsonl_data\n",
        "\n",
        "data = load_jsonl_data(\"../data/training/manual_test_set.jsonl\", isDataFrame=False)\n",
        "\n",
        "# Load the T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"../models/\" + input(\"Input the folder of the t5 model to be used\")\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", model_max_length=MAX_OUTPUT_TOKEN_LENGTH)\n",
        "\n",
        "for line in data:\n",
        "    # Preprocess input\n",
        "    input_text = \"summarize: \" + line[\"input\"]\n",
        "    expected_summary = line[\"output\"]\n",
        "\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        input_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
        "    )\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        # Adjust the max length according to your desired summary length\n",
        "        max_length=MAX_OUTPUT_TOKEN_LENGTH,\n",
        "        # Adjust the number of beams for beam search\n",
        "        num_beams=2,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"> INPUT TEXT: {input_text}\")\n",
        "    print(f\"\\t> EXPECTED SUMMARY: {expected_summary}\")\n",
        "    print(f\"\\t> GENERATE SUMMARY: {summary}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+sqU2Hgca8RM/Wjv+9kvQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "T5 Fine tuning with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
