{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Self-Written Fine Tuning Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Fine Tune Checkpoint Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: PyTorch with CPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "model_checkpoint = input(\"Input the t5 model checkpoint name to be fine tuned\")\n",
        "\n",
        "# Read in the JSONL training and validation data set\n",
        "data = pd.read_json(\"../data/training/training_validation_set.jsonl\", lines=True)\n",
        "# Prepend T5's summarize task keyword to inputs\n",
        "data[\"input\"] = \"summarize: \" + data[\"input\"]\n",
        "# Show random sample of 10 data sets\n",
        "data.sample(10)\n",
        "\n",
        "# Define a rich console logger\n",
        "console = Console(record=True)\n",
        "\n",
        "\n",
        "# Setup the data frame display\n",
        "def display_df(df):\n",
        "    console = Console()\n",
        "    table = Table(\n",
        "        Column(\"source_text\", justify=\"center\"),\n",
        "        Column(\"target_text\", justify=\"center\"),\n",
        "        title=\"Sample Data\",\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "    )\n",
        "\n",
        "    for _, row in enumerate(df.values.tolist()):\n",
        "        table.add_row(row[0], row[1])\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")\n",
        "\n",
        "# Setting up the device for GPU usage, if available\n",
        "from torch import cuda\n",
        "\n",
        "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# Creating a custom dataset for reading the dataset and loading it into the dataloader\n",
        "# to pass it to the neural network for fine tuning the model\n",
        "class YourDataSetClass(Dataset):\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # Cleaning data so as to ensure data is in string type\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "# Function to be called for training with the parameters passed from main function\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "            console.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# Function to evaluate model for predictions\n",
        "def validate(tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                max_length=150,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            preds = [\n",
        "                tokenizer.decode(\n",
        "                    g, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for g in generated_ids\n",
        "            ]\n",
        "            targets = [\n",
        "                tokenizer.decode(\n",
        "                    t, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                for t in y\n",
        "            ]\n",
        "            if _ % 10 == 0:\n",
        "                console.print(f\"Completed {_}\")\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(targets)\n",
        "    return predictions, actuals\n",
        "\n",
        "\n",
        "# T5 training main function\n",
        "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir):\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])\n",
        "    np.random.seed(model_params[\"SEED\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    # Tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"], model_max_length=512)\n",
        "\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of summary\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "    console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "    display_df(dataframe.head(2))\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # 80% of the data will be used for training and the rest for validation\n",
        "    train_size = 0.8\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    console.print(f\"VALIDATION Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of data loader\n",
        "    training_set = YourDataSetClass(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = YourDataSetClass(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of data loaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of data loaders for testing and validation - this will be used down for training and validation stage for the model\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    console.log(f\"[Saving Model]...\\n\")\n",
        "    # Saving the model after training\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # Evaluating validation dataset\n",
        "    console.log(f\"[Initiating Validation]...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "\n",
        "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
        "\n",
        "    console.log(f\"[Validation Completed.]\\n\")\n",
        "    console.print(\n",
        "        f\"\"\"[Model] Model saved @ {output_dir}\\n\"\"\"\n",
        "    )\n",
        "    console.print(\n",
        "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
        "    )\n",
        "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    # model_type: t5-x\n",
        "    \"MODEL\": f\"{model_checkpoint}\",\n",
        "    # training batch size\n",
        "    \"TRAIN_BATCH_SIZE\": 8,\n",
        "    # validation batch size\n",
        "    \"VALID_BATCH_SIZE\": 60,\n",
        "    # number of training epochs\n",
        "    \"TRAIN_EPOCHS\": 1,\n",
        "    # number of validation epochs\n",
        "    \"VAL_EPOCHS\": 1,\n",
        "    # learning rate\n",
        "    \"LEARNING_RATE\": 1e-4,\n",
        "    # max length of source text\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 512,\n",
        "    # max length of target text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 50,\n",
        "    # set seed for reproducibility\n",
        "    \"SEED\": 42,\n",
        "}\n",
        "output_directory = f\"../models/pytorch-cpu-{model_checkpoint}\"\n",
        "\n",
        "# Run training function on the T5 model using data set and training parameters\n",
        "T5Trainer(\n",
        "    dataframe=data,\n",
        "    source_text=\"input\",\n",
        "    target_text=\"output\",\n",
        "    model_params=model_params,\n",
        "    output_dir=f\"{output_directory}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: TensorFlow Metal with GPU \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "2023-07-15 00:55:53.249913: W tensorflow/core/framework/op_kernel.cc:1816] INVALID_ARGUMENT: ValueError: Input b'summarize: As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.' is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 266, in __call__\n",
            "    return func(device, token, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 144, in __call__\n",
            "    outputs = self._call(device, args)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 151, in _call\n",
            "    ret = self._func(*args)\n",
            "          ^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/var/folders/f1/_vqvbs517hn1khv6hl8jvy1w0000gn/T/ipykernel_65663/2820134923.py\", line 30, in encode\n",
            "    inputs = tokenizer.encode(\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2206, in encode\n",
            "    encoded_inputs = self.encode_plus(\n",
            "                     ^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2536, in encode_plus\n",
            "    return self._encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 647, in _encode_plus\n",
            "    first_ids = get_input_ids(text)\n",
            "                ^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 634, in get_input_ids\n",
            "    raise ValueError(\n",
            "\n",
            "ValueError: Input b'summarize: As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.' is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\n",
            "\n",
            "\n",
            "2023-07-15 00:55:53.251087: W tensorflow/core/framework/op_kernel.cc:1816] INVALID_ARGUMENT: ValueError: Input b'summarize: I took charge as the fiscal watchdog and successfully revitalized the residue program. By returning unused vehicle parts to local vendors, I recouped $4,000 in funds, demonstrating my dedication to efficient financial management.' is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 266, in __call__\n",
            "    return func(device, token, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 144, in __call__\n",
            "    outputs = self._call(device, args)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 151, in _call\n",
            "    ret = self._func(*args)\n",
            "          ^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/var/folders/f1/_vqvbs517hn1khv6hl8jvy1w0000gn/T/ipykernel_65663/2820134923.py\", line 30, in encode\n",
            "    inputs = tokenizer.encode(\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2206, in encode\n",
            "    encoded_inputs = self.encode_plus(\n",
            "                     ^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2536, in encode_plus\n",
            "    return self._encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 647, in _encode_plus\n",
            "    first_ids = get_input_ids(text)\n",
            "                ^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 634, in get_input_ids\n",
            "    raise ValueError(\n",
            "\n",
            "ValueError: Input b'summarize: I took charge as the fiscal watchdog and successfully revitalized the residue program. By returning unused vehicle parts to local vendors, I recouped $4,000 in funds, demonstrating my dedication to efficient financial management.' is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Input b'summarize: As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.' is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\nTraceback (most recent call last):\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 266, in __call__\n    return func(device, token, args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 144, in __call__\n    outputs = self._call(device, args)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 151, in _call\n    ret = self._func(*args)\n          ^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/var/folders/f1/_vqvbs517hn1khv6hl8jvy1w0000gn/T/ipykernel_65663/2820134923.py\", line 30, in encode\n    inputs = tokenizer.encode(\n             ^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2206, in encode\n    encoded_inputs = self.encode_plus(\n                     ^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2536, in encode_plus\n    return self._encode_plus(\n           ^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 647, in _encode_plus\n    first_ids = get_input_ids(text)\n                ^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 634, in get_input_ids\n    raise ValueError(\n\nValueError: Input b'summarize: As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.' is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name: ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     92\u001b[0m total_val_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 94\u001b[0m \u001b[39mfor\u001b[39;00m batch, (inp, tar) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data\u001b[39m.\u001b[39mbatch(BATCH_SIZE)):\n\u001b[1;32m     95\u001b[0m     batch_loss \u001b[39m=\u001b[39m train_step(inp, tar)\n\u001b[1;32m     96\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\n",
            "File \u001b[0;32m~/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:814\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    813\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    815\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    816\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:777\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 777\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    778\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    779\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    780\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    782\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3028\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3027\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 3028\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   3029\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   3030\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Input b'summarize: As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.' is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\nTraceback (most recent call last):\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 266, in __call__\n    return func(device, token, args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 144, in __call__\n    outputs = self._call(device, args)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 151, in _call\n    ret = self._func(*args)\n          ^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/var/folders/f1/_vqvbs517hn1khv6hl8jvy1w0000gn/T/ipykernel_65663/2820134923.py\", line 30, in encode\n    inputs = tokenizer.encode(\n             ^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2206, in encode\n    encoded_inputs = self.encode_plus(\n                     ^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2536, in encode_plus\n    return self._encode_plus(\n           ^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 647, in _encode_plus\n    first_ids = get_input_ids(text)\n                ^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/justin/Documents/Code/personal/smarter-bullets/.venv/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 634, in get_input_ids\n    raise ValueError(\n\nValueError: Input b'summarize: As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.' is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "import pandas as pd\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "\n",
        "console = Console()\n",
        "model_checkpoint = input(\"Input the t5 model checkpoint name to be fine tuned\")\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Read in the JSONL training and validation data set\n",
        "data = pd.read_json(\"../data/training/training_validation_set.jsonl\", lines=True)\n",
        "# Prepend T5's summarize task keyword to inputs\n",
        "data[\"input\"] = \"summarize: \" + data[\"input\"]\n",
        "# Show random sample of 10 data sets\n",
        "data.sample(10)\n",
        "\n",
        "# Define the table\n",
        "table = Table(show_header=True, header_style=\"bold magenta\")\n",
        "table.add_column(\"Epoch\")\n",
        "table.add_column(\"Train Loss\")\n",
        "table.add_column(\"Val Loss\")\n",
        "\n",
        "\n",
        "# Load and encode the data\n",
        "def encode(input_text, output_text):\n",
        "    inputs = tokenizer.encode(\n",
        "        input_text, return_tensors=\"tf\", truncation=True, padding=True\n",
        "    )\n",
        "    targets = tokenizer.encode(\n",
        "        output_text, return_tensors=\"tf\", truncation=True, padding=True\n",
        "    )\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "def tf_encode(input_text, target_text):\n",
        "    input_text = tf.compat.as_text(input_text.numpy())\n",
        "    target_text = tf.compat.as_text(target_text.numpy())\n",
        "\n",
        "    inputs = tokenizer.encode(\n",
        "        input_text, return_tensors=\"tf\", truncation=True, padding=True\n",
        "    )\n",
        "    targets = tokenizer.encode(\n",
        "        target_text, return_tensors=\"tf\", truncation=True, padding=True\n",
        "    )\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Convert DataFrame to TensorFlow Dataset\n",
        "data_tf = tf.data.Dataset.from_tensor_slices(\n",
        "    (data[\"input\"].values, data[\"output\"].values)\n",
        ")\n",
        "\n",
        "# Apply encoding to TensorFlow Dataset\n",
        "data = data_tf.map(tf_encode)\n",
        "\n",
        "\n",
        "# Split the data into training and validation\n",
        "tf.random.set_seed(0)\n",
        "data = data.shuffle(1000)\n",
        "train_data = data.take(int(0.8 * len(data)))\n",
        "val_data = data.skip(int(0.8 * len(data)))\n",
        "\n",
        "# Define the loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "# Define the train step function\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(\n",
        "            inp,\n",
        "            attention_mask=inp != 0,\n",
        "            decoder_input_ids=tar[:, :-1],\n",
        "            labels=tar[:, 1:],\n",
        "        )[1]\n",
        "        loss = loss_object(tar[:, 1:], predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Train and validate the model\n",
        "EPOCHS = 5  # Set the number of epochs\n",
        "BATCH_SIZE = 8  # Lower the batch size for small datasets\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    for batch, (inp, tar) in enumerate(train_data.batch(BATCH_SIZE)):\n",
        "        batch_loss = train_step(inp, tar)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    for batch, (inp, tar) in enumerate(val_data.batch(BATCH_SIZE)):\n",
        "        predictions = model(\n",
        "            inp,\n",
        "            attention_mask=inp != 0,\n",
        "            decoder_input_ids=tar[:, :-1],\n",
        "            labels=tar[:, 1:],\n",
        "        )[1]\n",
        "        val_loss = loss_object(tar[:, 1:], predictions)\n",
        "        total_val_loss += val_loss\n",
        "\n",
        "    table.add_row(\n",
        "        str(epoch + 1),\n",
        "        f\"{total_loss / (batch + 1):.4f}\",\n",
        "        f\"{total_val_loss / (batch + 1):.4f}\",\n",
        "    )\n",
        "\n",
        "console.print(table)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(f\"../models/tensorflow-gpu-{model_checkpoint}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fine Tuned Model Manual Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    input(\"Input the directory path of the t5 model to be used\")\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(input(\"Input the t5 model name being loaded\"), model_max_length=512)\n",
        "\n",
        "# Preprocess input\n",
        "input_text = input(\"Insert a statement to summarize\")\n",
        "input_expected_summary = input(\"Insert the expected statement summary\")\n",
        "prefix = \"summarize: \"\n",
        "input_text = prefix + input_text\n",
        "\n",
        "inputs = tokenizer.encode_plus(\n",
        "    input_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
        ")\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    # Adjust the max length according to your desired summary length\n",
        "    max_length=150,\n",
        "    # Adjust the number of beams for beam search\n",
        "    num_beams=20,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"> Expected Summary: \", input_expected_summary)\n",
        "print(\"> Generated Summary: \", summary)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM+sqU2Hgca8RM/Wjv+9kvQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "T5 Fine tuning with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
