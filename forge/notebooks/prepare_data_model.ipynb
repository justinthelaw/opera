{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fine Tuning Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Flow\n",
    "\n",
    "- This notebook is used for generating the JSONL files used to fine-tune a model. \n",
    "- The goal is to use [scrape.py](./scripts/scrape.py) to scrape for open-source bullets at a particular URL and use those as expected model completions. \n",
    "- These expected model completions have inputs that are generated using through the use of another model's generative capabilities, expanding the acronyms and adding context to the bullet in order to mimic natural human language input. \n",
    "- Once the inputs and expected completions are gathered and cleaned, they can be used to train the Bullet Forge's model.\n",
    "\n",
    "The steps for this notebook are as follows:\n",
    "- [Step 1: Run the Bullet Scraper](#step-1-run-the-bullet-scraper)\n",
    "- [Step 2: Clean the Scraped Data](#step-2-clean-the-scraped-data)\n",
    "- [Step 3: Consolidate the Cleaned Data](#step-3-consolidate-the-cleaned-data)\n",
    "- [Step 4: Generating Inputs for Completions](#step-4-generating-inputs-for-completions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run the Bullet Scraper\n",
    "\n",
    "This step retrieves \"expected completions\", bullets, from a variety of sources in order to form the foundation of the training data.\n",
    "\n",
    "Please see the [scripts/](./scripts) directory for more details. Please be warned that the scraper, although parallelized, may take a long time to run through all of the identified bullet repositories. The block below runs the scraper on all the easily scrape-able websites located within the [websites.txt](../resources/websites.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "with open('../resources/websites.txt', 'r') as file:\n",
    "    urls = [url.strip() for url in file.readlines()]\n",
    "\n",
    "for url in urls:\n",
    "    subprocess.run([\"python3\", \"scripts/scrape.py\", url])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below is **_OPTIONAL_**. It allows you to run the scraper one websites at a time. This is useful for the website(s) below, where there is a hard-coded records limit. Please note that this is a very poorly crafted cronjob, so once success has been logged the first few times on a website, wait 30 more seconds and then stop the script. If you don't stop the script manually, you may end up waiting a really long time for the script to shutdown. The block requires the following input:\n",
    "\n",
    "- BASE_URL: The base URL including the protocol, e.g., https://www.afeprbullets.com/results.php?Submit5=Search&strength=Positive&rec=8124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = input(\"Enter a base url to scrape from\")\n",
    "\n",
    "!python3 scripts/scrape.py \"{BASE_URL}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Scraped Data\n",
    "\n",
    "In this step, the scraped data is given some extra cleaning. This removes bullets that are clearly way too long or short, and provides proper spacing and formatting to bullets that do not follow the standard bullet format. The block below requires the following input:\n",
    "\n",
    "- DIRTY_FILE: The name of the text file you want cleaned, from the [data/raw/](../data/raw/) directory, e.g., `afeprbullets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRTY_FILE = input(\"Enter a filename to clean\")\n",
    "\n",
    "!python3 scripts/clean.py \"{DIRTY_FILE}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Consolidate the Cleaned Data\n",
    "\n",
    "In this step, all of the expected completions are consolidated into one JSONL file for easier handling in later steps and notebooks. The JSON objects within the JSONL will carry the following structure: \n",
    "\n",
    "```json\n",
    "{\"input\": \"<ADD DETAIL>\", \"output\": \"{line}\"}\n",
    "```\n",
    "\n",
    "The `output` key stores the expected completion and the `input` key stores the prompt. `<ADD DETAIL>` is the location in which the `output` requires the Bullet Forge user's input. [Step 4](#step-4-generating-inputs-for-completions) talks more about the generation of the entire `input` key's value.\n",
    "\n",
    "The block below does not require any inputs; however, do note that the output of the consolidated data is within the variable `directory_path` below, and the name of the output file will always be `consolidated_set.jsonl`, unless the user explicitly changes it within the [consolidate.py](./scripts/consolidate.py) script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory_path = \"../data/raw/\"\n",
    "\n",
    "file_paths = [os.path.join(root, file) for root, _, files in os.walk(directory_path) for file in files]\n",
    "\n",
    "file_paths_string = \" \".join(file_paths)\n",
    "\n",
    "!python3 scripts/consolidate.py {file_paths_string}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below allows the user to specify a new or existing file to be consolidated to a different file, and not the user's existing, master copy of `consolidated_set.jsonl`.\n",
    "- FILENAME: The name of the file to be consolidated and formatted, e.g. `contributed`\n",
    "- OUTPUT_FILENAME: The of the file not including the extension, e.g., `contributed_consolidated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-12 22:43:34.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.file_utils\u001b[0m:\u001b[36mfile_exists\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mPrinting all lines to file path: ../data/raw/contributed.jsonl\u001b[0m\n",
      "\u001b[32m2023-07-12 22:43:34.277\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mconsolidate\u001b[0m:\u001b[36mprocess_file\u001b[0m:\u001b[36m35\u001b[0m - \u001b[32m\u001b[1mFile added to consolidation: ../data/raw/contributed.txt\u001b[0m\n",
      "\u001b[32m2023-07-12 22:43:34.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.file_utils\u001b[0m:\u001b[36mremove_duplicates\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mCleaning up duplicates...\u001b[0m\n",
      "\u001b[32m2023-07-12 22:43:34.278\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mutils.file_utils\u001b[0m:\u001b[36mremove_duplicates\u001b[0m:\u001b[36m65\u001b[0m - \u001b[32m\u001b[1mOutput file's duplicate bullets removed!\u001b[0m\n",
      "\u001b[32m2023-07-12 22:43:34.278\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mconsolidate\u001b[0m:\u001b[36mconsolidate_files\u001b[0m:\u001b[36m67\u001b[0m - \u001b[32m\u001b[1mConsolidated, clean data has been output to: ../data/raw/contributed.jsonl\u001b[0m\n",
      "\u001b[32m2023-07-12 22:43:34.279\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mconsolidate_one_file\u001b[0m:\u001b[36m10\u001b[0m - \u001b[32m\u001b[1mConsolidated, clean data has been output to: ../data/raw/contributed.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "FILENAME = input(\"Insert the file for consolidation\")\n",
    "OUTPUT_FILENAME = input(\"Insert the file for consolidation\")\n",
    "\n",
    "file_path = f\"../data/raw/{FILENAME}.txt\"\n",
    "output_path =  f\"../data/raw/{OUTPUT_FILENAME}.jsonl\"\n",
    "\n",
    "!python3 scripts/consolidate_one.py \"{file_path}\" \"{output_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generating Inputs for Completions\n",
    "\n",
    "As discussed in [Step 3](#step-3-consolidate-the-cleaned-data), the bullet scraping, cleaning, and consolidating do not yield the inputs that an actual user of Bullet Forge may provide. The `<ADD DETAIL>` is purposefully left within each `input` so that the JSONL's JSON objects can be fed directly into a prompt-engineered model that can expand the bullet back into natural human language. \n",
    "\n",
    "Below is an example of a completed JSON object for training. Please note that the JSON object below is pretty-formatted for ease of viewing, but in reality, the JSONL will be flat.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"input\": \"As the Subject Matter Expert (SME) for the Exceptional Family Member Program (EFMP), I expertly directed 57 enrollments, handled 194 incoming inquiries, and addressed 101 outgoing inquiries. Through my leadership and efficient processing, we beat the package processing time by 50% and achieved an on-time rate of 99%, significantly improving support for our EFMP families.\",\n",
    "    \"output\": \"- EFMP SME; dir'd 57 enrollments/194 incoming/101 outgoing inquiries--beat pkg processing time 50%/on-time rt 99%\"\n",
    "}\n",
    "```\n",
    "\n",
    "Any model or tool can be used to get the input-output JSON object. For example, you can prompt-engineer ChatGPT using the below:\n",
    "\n",
    "```\n",
    "INSTRUCTIONS: Expand upon condensed information that follows the bullet format `-<ACTION>;<IMPACT>--<OUTCOME>`. Within areas of `<ADD DETAIL>` the task is to take the bullet after \"output\" and:\n",
    "1. expand all acronyms or non-standard english words into their original forms, \n",
    "2. provide more context and language to generate full-form sentences that amount to a small paragraph describing the bullet, \n",
    "3. generate the context and language as if the perspective were from that of a member of the military, \n",
    "4. replace the `<ADD DETAIL>` area in the JSONL JSON-object\n",
    "\n",
    "EXAMPLE OUTPUT BELOW:\n",
    "\n",
    "MY PROMPT: `{\"input\": \"<ADD DETAIL>\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`\n",
    "\n",
    "YOUR RESPONSE: `{\"input\": \"As the ESD focal point, I played a crucial role in tracking, routing, and resolving 375 Tier III and 6 high-level tickets. My efforts enabled AFNET access for 200,000 users, ensuring smooth operations and connectivity.\", \"output\": \"- ESD focal point; tracked, routed, resolved 375 Tier III/6 High lvl tkts--enabled AFNET access to 200K users\"}`. \n",
    "\n",
    "IMPORTANT NOTE: Take the expansions and replace the `<ADD DETAIL>` area in the JSON. If provided multiple JSONS with `<ADD DETAIL>` areas, then iterate through them performing the expansion. Return all of your results in a code block of type JSON, and have each expansion stays on 1 line of the overall code block. \n",
    "\n",
    "ACTION: Please provide an acknowledgement and summary of the instructions above if you understand the upcoming task.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
