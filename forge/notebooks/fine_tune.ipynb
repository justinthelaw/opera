{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Model Fine-tuning\n",
    "\n",
    "This notebook is used for fine-tuning the T5 base model. Please refer to the `README.md` within the parent `forge/` directory for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read, Tokenize and Encode Data\n",
    "\n",
    "The block below reads data to memory and performs tokenization on all IO for the fine tuning process. \n",
    "\n",
    "- TRAINING_FILE: The name of the JSONL file, from the `data/training` directory\n",
    "- MAXIMUM_SIZE: The maximum size of the data you want to read to memory, where `0` extracts all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-09 20:48:33.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mSample of encoded data: {'input_ids': tensor([[282,   8, 262,  ...,   0,   0,   0]]), 'labels': tensor([[  3,  18, 262,  ...,   0,   0,   0]])}\u001b[0m\n",
      "\u001b[32m2023-07-09 20:48:33.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mCount of encoded data: 50\u001b[0m\n",
      "\u001b[32m2023-07-09 20:48:33.142\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[32m\u001b[1mThe data has been tokenized and encoded into memory!\u001b[0m\n",
      "\u001b[32m2023-07-09 20:48:33.143\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[33m\u001b[1mThis tokenized and encoded data is only temporarily stored in the Jupyter Notebook instance.\u001b[0m\n",
      "\u001b[32m2023-07-09 20:48:33.144\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[33m\u001b[1mFailing to save the data to file wil result in loss during restart or clearing of outputs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from loguru import logger\n",
    "from scripts.utils.file_utils import jsonl_read\n",
    "from scripts.prepare_training import tokenize_and_encode\n",
    "\n",
    "async def main():\n",
    "    # Read in the training data JSONL file\n",
    "    TRAINING_FILE = input(\"Enter the JSONL filename: \")\n",
    "    MAXIMUM_SIZE = input(\"Enter a maximum data read size: \")\n",
    "    training_file_path = f\"../data/training/{TRAINING_FILE}\"\n",
    "    data = await jsonl_read(training_file_path, int(MAXIMUM_SIZE))\n",
    "    if data == []:\n",
    "        logger.error(f\"An error occurred during the reading of JSONL file: {training_file_path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Tokenize and encode each IO pair\n",
    "    return await tokenize_and_encode(data)\n",
    "\n",
    "# Run the async events\n",
    "def run_asyncio_loop():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(main())\n",
    "\n",
    "\n",
    "# Enable nested event loops\n",
    "nest_asyncio.apply()\n",
    "prepared_data = run_asyncio_loop()\n",
    "\n",
    "logger.info(f\"Sample of the tokenized and encoded data: {prepared_data[0]}\")\n",
    "logger.info(f\"Total count of tokenized and encoded data: {len(prepared_data)}\")\n",
    "logger.success(f\"The data has been tokenized and encoded into memory!\")\n",
    "logger.warning(f\"This tokenized and encoded data is only temporarily stored in the Jupyter Notebook instance.\")\n",
    "logger.warning(f\"Failing to save the data to file wil result in loss during restart or clearing of outputs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
