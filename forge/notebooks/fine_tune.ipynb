{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Pre-Trained Model\n",
    "\n",
    "This notebook is used for fine-tuning the T5 pre-trained model. Please refer to the `README.md` within the parent `forge/` directory for more details.\n",
    "\n",
    "Potential models for use:\n",
    "- sysresearch101/t5-large-finetuned-xsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read, Tokenize and Encode Data\n",
    "\n",
    "The block below reads data to memory and performs tokenization on all IO for the fine tuning process. \n",
    "\n",
    "- TRAINING_FILE (`training_set.jsonl`): The name of the JSONL file, from the `data/training` directory, that will be used for training.\n",
    "- MAXIMUM_SIZE (`0`): The maximum size of the data you want to read to memory, where an input of `0` extracts all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-10 22:55:59.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscripts.tokenize_and_encode\u001b[0m:\u001b[36mtokenize_and_encode\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mSample of the tokenized and encoded data: {'input_ids': tensor([282,   8, 262,  ...,   0,   0,   0]), 'labels': tensor([  3,  18, 262,  ...,   0,   0,   0])}\u001b[0m\n",
      "\u001b[32m2023-07-10 22:55:59.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscripts.tokenize_and_encode\u001b[0m:\u001b[36mtokenize_and_encode\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mTotal count of tokenized and encoded data: 50\u001b[0m\n",
      "\u001b[32m2023-07-10 22:55:59.936\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mscripts.tokenize_and_encode\u001b[0m:\u001b[36mtokenize_and_encode\u001b[0m:\u001b[36m28\u001b[0m - \u001b[32m\u001b[1mThe data has been tokenized and encoded into memory!\u001b[0m\n",
      "\u001b[32m2023-07-10 22:55:59.937\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mscripts.tokenize_and_encode\u001b[0m:\u001b[36mtokenize_and_encode\u001b[0m:\u001b[36m29\u001b[0m - \u001b[33m\u001b[1mThis tokenized and encoded data is only temporarily stored in the Jupyter Notebook instance.\u001b[0m\n",
      "\u001b[32m2023-07-10 22:55:59.937\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mscripts.tokenize_and_encode\u001b[0m:\u001b[36mtokenize_and_encode\u001b[0m:\u001b[36m32\u001b[0m - \u001b[33m\u001b[1mFailing to save the data to file will result in loss during restart or clearing of outputs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from transformers import T5Tokenizer\n",
    "from scripts.tokenize_encode import tokenize_and_encode\n",
    "\n",
    "\n",
    "# Preprocess the training data using the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", model_max_length=1024)\n",
    "\n",
    "\n",
    "# Run the async events\n",
    "async def main():\n",
    "    # Read in the training data JSONL file\n",
    "    TRAINING_FILE = \"training_set.jsonl\"\n",
    "    MAXIMUM_SIZE = 0\n",
    "\n",
    "    # Tokenize and encode each IO pair\n",
    "    return await tokenize_and_encode(TRAINING_FILE, int(MAXIMUM_SIZE), tokenizer)\n",
    "\n",
    "\n",
    "# Enable nested event loops\n",
    "nest_asyncio.apply()\n",
    "prepared_data = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Training Parameters\n",
    "\n",
    "The parameters below have their recommended default values in parenthesis. Changing the value may result in different trained model variations. Feel free to experiment with each parameter based on the locally available datasets, compute and store limitations, and other factors.\n",
    "\n",
    "- EPOCHS (`3`): The number of times the training loop will iterate over the entire dataset. Increasing the number of epochs can potentially improve the model's performance by allowing it to see the data multiple times, but too many epochs may lead to over-fitting.\n",
    "- LEARNING_RATE (`2e-5`): The step size at which the optimizer adjusts the model's parameters during training. A higher learning rate can result in faster convergence but may cause instability or overshooting. A lower learning rate can lead to slower convergence but more stable training.\n",
    "- TOTAL_STEPS (`len(prepared_data) * EPOCHS`): The total number of steps to train the model. It is calculated by multiplying the number of training examples with the number of epochs. Increasing the total steps can provide more training iterations, potentially allowing the model to learn more from the data, but it also increases the training time and computational resources required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training parameters\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "TOTAL_STEPS = len(prepared_data) * EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Model\n",
    "\n",
    "Using the previous parameters, the model will be trained with the prepared data. For more details, please see the `scripts/` directory.\n",
    "\n",
    "Learning rate scheduling strategy notes:\n",
    "\n",
    "- max_lr: The maximum learning rate used during training - helps control the learning rate range during the training process\n",
    "- total_steps: The total number of steps in the training process - influences the scheduling of the learning rate and momentum during training\n",
    "- div_factor: The factor by which the initial learning rate is divided to get the lower boundary learning rate - affects the lower bound of the learning rate range\n",
    "- final_div_factor: The factor by which the initial learning rate is divided to get the final learning rate - affects the final learning rate at the end of the training process\n",
    "- pct_start: The percentage of the total number of steps used for the warm-up phase - determines the portion of the training where the learning rate gradually increases\n",
    "- anneal_strategy: The strategy used for annealing the learning rate and momentum during training - set to \"cos\" for cosine annealing\n",
    "- cycle_momentum: Whether to cycle the momentum between base_momentum and max_momentum during training\n",
    "- base_momentum: The lower momentum boundary during training\n",
    "- max_momentum: The upper momentum boundary during training\n",
    "- epochs: The number of epochs to train the model\n",
    "- steps_per_epoch: The number of steps per epoch - used to calculate the learning rate schedule\n",
    "- warmup_steps: The number of warm-up steps where the learning rate gradually increases - helps the model to stabilize at the beginning of training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training parameters\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "TOTAL_STEPS = len(prepared_data) * EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Model\n",
    "\n",
    "Using the previous parameters, the model will be trained with the prepared data. For more details, please see the `scripts/` directory.\n",
    "\n",
    "Learning rate scheduling strategy notes:\n",
    "\n",
    "- max_lr: The maximum learning rate used during training - helps control the learning rate range during the training process\n",
    "- total_steps: The total number of steps in the training process - influences the scheduling of the learning rate and momentum during training\n",
    "- div_factor: The factor by which the initial learning rate is divided to get the lower boundary learning rate - affects the lower bound of the learning rate range\n",
    "- final_div_factor: The factor by which the initial learning rate is divided to get the final learning rate - affects the final learning rate at the end of the training process\n",
    "- pct_start: The percentage of the total number of steps used for the warm-up phase - determines the portion of the training where the learning rate gradually increases\n",
    "- anneal_strategy: The strategy used for annealing the learning rate and momentum during training - set to \"cos\" for cosine annealing\n",
    "- cycle_momentum: Whether to cycle the momentum between base_momentum and max_momentum during training\n",
    "- base_momentum: The lower momentum boundary during training\n",
    "- max_momentum: The upper momentum boundary during training\n",
    "- epochs: The number of epochs to train the model\n",
    "- steps_per_epoch: The number of steps per epoch - used to calculate the learning rate schedule\n",
    "- warmup_steps: The number of warm-up steps where the learning rate gradually increases - helps the model to stabilize at the beginning of training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/7 [00:00<?, ?batch/s]/var/folders/f1/_vqvbs517hn1khv6hl8jvy1w0000gn/T/ipykernel_55487/3740164190.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(input_ids, dtype=torch.long).clone().detach()\n",
      "/var/folders/f1/_vqvbs517hn1khv6hl8jvy1w0000gn/T/ipykernel_55487/3740164190.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels, dtype=torch.long).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "class PreparedDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx][\"input_ids\"]\n",
    "        labels = self.data[idx][\"labels\"]\n",
    "        input_ids_tensor = input_ids.clone().detach()\n",
    "        labels_tensor = labels.clone().detach()\n",
    "        return {\n",
    "            \"input_ids\": input_ids_tensor,\n",
    "            \"labels\": labels_tensor,\n",
    "        }\n",
    "\n",
    "\n",
    "def train_t5_model(model, data_loader, optimizer, scheduler, device, epochs):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\")\n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "                batch_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=batch_input_ids, labels=batch_labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"An error occurred during training: {e}\")\n",
    "\n",
    "        average_loss = total_loss / len(data_loader)\n",
    "        logger.info(f\"Epoch {epoch + 1} - Average Loss: {average_loss}\")\n",
    "\n",
    "\n",
    "# Convert the dataset into a PyTorch DataLoader\n",
    "batch_size = 8\n",
    "dataset = PreparedDataset(prepared_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    div_factor=2.0,\n",
    "    final_div_factor=1e4,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy=\"cos\",\n",
    "    cycle_momentum=True,\n",
    "    base_momentum=0.85,\n",
    "    max_momentum=0.95,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(data_loader),\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_t5_model(model, data_loader, optimizer, scheduler, device, EPOCHS)\n",
    "\n",
    "# Save the trained model\n",
    "output_model = \"../models/t5/trained\"\n",
    "model.save_pretrained(output_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Fine-Tuned T5 Model\n",
    "\n",
    "The block below provides validation of the fine-tuned T5 model using a set of data and outputs, where the outputs can be visually inspected by a user. The parameters from the defaults can be changed to the local development environment's specific or available models and datasets.\n",
    "- FINE_TUNED_MODEL (`../models/t5/trained`): The T5 model to be used for the input and output generation task.\n",
    "- INPUT_TEXT_ARRAY (`../data/training/validation_set.jsonl`): A validated set of summary-evaluation pairs for visual inspection. The default file has 20 pairs for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from scripts.utils.file_utils import jsonl_read\n",
    "\n",
    "# Load the saved fine-tuned model\n",
    "fine_tuned_model = T5ForConditionalGeneration.from_pretrained(\"../models/t5/base\")\n",
    "\n",
    "# Set the device for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fine_tuned_model.to(device)\n",
    "\n",
    "# Use the model on the validation data sets\n",
    "input_pairs_array = await jsonl_read(\"../data/training/validation_set.jsonl\")\n",
    "for key, input_pairs in enumerate(input_pairs_array):\n",
    "    input_text = input_pairs[\"summary\"]\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    outputs = fine_tuned_model.generate(input_ids, max_length=512)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    expected_text = input_pairs[\"evaluation\"]\n",
    "    print(\n",
    "        f\"({key + 1}) Input Text: {input_text}\\n\\tExpected Text: {expected_text}\\n\\tGenerated Text: {generated_text}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
