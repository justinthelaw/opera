{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Model Fine-tuning\n",
    "\n",
    "This notebook is used for fine-tuning the T5 base model. Please refer to the `README.md` within the parent `forge/` directory for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read, Tokenize and Encode Data\n",
    "\n",
    "The block below reads data to memory and performs tokenization on all IO for the fine tuning process. \n",
    "\n",
    "- TRAINING_FILE: The name of the JSONL file, from the `data/training` directory\n",
    "- MAXIMUM_SIZE: The maximum size of the data you want to read to memory, where `0` extracts all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from loguru import logger\n",
    "from scripts.utils.file_utils import jsonl_read\n",
    "from scripts.prepare_training import tokenize_and_encode\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Read in the training data JSONL file\n",
    "    TRAINING_FILE = input(\"Enter the JSONL filename: \")\n",
    "    MAXIMUM_SIZE = input(\"Enter a maximum data read size: \")\n",
    "    training_file_path = f\"../data/training/{TRAINING_FILE}\"\n",
    "    data = await jsonl_read(training_file_path, int(MAXIMUM_SIZE))\n",
    "    if data == []:\n",
    "        logger.error(\n",
    "            f\"An error occurred during the reading of JSONL file: {training_file_path}\"\n",
    "        )\n",
    "        exit(1)\n",
    "\n",
    "    # Tokenize and encode each IO pair\n",
    "    return await tokenize_and_encode(data)\n",
    "\n",
    "\n",
    "# Run the async events\n",
    "def run_asyncio_loop():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(main())\n",
    "\n",
    "\n",
    "# Enable nested event loops\n",
    "nest_asyncio.apply()\n",
    "prepared_data = run_asyncio_loop()\n",
    "\n",
    "logger.info(f\"Sample of the tokenized and encoded data: {prepared_data[0]}\")\n",
    "logger.info(f\"Total count of tokenized and encoded data: {len(prepared_data)}\")\n",
    "logger.success(f\"The data has been tokenized and encoded into memory!\")\n",
    "logger.warning(\n",
    "    f\"This tokenized and encoded data is only temporarily stored in the Jupyter Notebook instance.\"\n",
    ")\n",
    "logger.warning(\n",
    "    f\"Failing to save the data to file wil result in loss during restart or clearing of outputs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from loguru import logger\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Configure the training parameters\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 1000\n",
    "total_steps = len(prepared_data) * epochs\n",
    "\n",
    "# Prepare the input tensors\n",
    "input_ids = torch.stack([item[\"input_ids\"] for item in prepared_data])\n",
    "labels = torch.stack([item[\"labels\"] for item in prepared_data])\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = TensorDataset(input_ids, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        logger.info(f\"Epoch {epoch + 1} - Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"An error occurred during training: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "try:\n",
    "    model.save_pretrained(\"../models/t5/trained/model-100\")\n",
    "    logger.success(\"Successfully saved t5-base model!\")\n",
    "except Exception as e:\n",
    "    logger.exception(f\"An error occurred while saving the model: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
